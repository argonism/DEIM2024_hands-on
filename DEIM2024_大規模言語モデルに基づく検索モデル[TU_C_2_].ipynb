{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozXAprDmskva"
      },
      "source": [
        "#\n",
        "# DEIM 2024 チュートリアル **大規模言語モデルに基づく検索モデル** \\[TU-C-2\\]\n",
        "\n",
        "## 概要\n",
        "近年の大規模言語モデルに基づく検索モデルを用いた検索実験のデモ\n",
        "\n",
        "DPRの学習と評価のデモを通して、よく用いられているフレームワークやツールを示します．\n",
        "\n",
        "ここでは，**NFCorpus**[1]というデータセットを用いて，DPRというBERTベースの密検索モデルを訓練します．\n",
        "1. ファインチューニングする前に、DPRのnfcorpusでの性能を評価します．\n",
        "1. nfcorpusの訓練セットでDPRを訓練します．\n",
        "1. 2.で得られたDPRを再びnfcorpusで評価し，DPRのin-domainの検索性能を確認します．\n",
        "\n",
        "[1]: Boteva et al. A Full-Text Learning to Rank Dataset for Medical Information Retrieval. ECIR, 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx8iVg1l5Aa6"
      },
      "source": [
        "## 始める前に\n",
        "**ランタイムのタイプがGPU（e.g. T4 GPU）になっていることを確認してください！**\n",
        "\n",
        "確認方法\n",
        "- colab上部のナビゲーションバーから「ランタイム」> 「ランタイムのタイプを変更」\n",
        "- 「ハードウェア アクセラレータ」の指定がGPUになっていれば閉じる。\n",
        "- なっていなければ選択できるGPUを指定して「保存」を押す"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdp0cI1ztaOz"
      },
      "source": [
        "## 依存関係のインストール\n",
        "\n",
        "今回は、DPRのファインチューニングにTevatronという大規模言語モデルの訓練ツールキットを使います。\n",
        "\n",
        "そのため、まずはTevatronと、依存パッケージをインストールします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-oPgZDg6Eq4"
      },
      "source": [
        "### **Tevatron**\n",
        "\n",
        "大規模言語モデルに基づく検索モデルの学習や評価に焦点を当てたツールキット・pythonフレームワークです。\n",
        "\n",
        "情報検索系のフレームワークの中では大規模言語モデル系の検索モデルの学習に強いという特徴があり、Tevatronを用いて検索モデルの訓練を行った研究も多数存在しています。\n",
        "\n",
        "- Gao et al. Tevatron: An efficient and flexible toolkit for dense retrieval. SIGIR, 2023.\n",
        "- Zhao et al. Dense Text Retrieval based on Pretrained Language Models: A Survey, arXiv, 2022.\n",
        "- Zhuang eet al. CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos. SIGIR, 2022.\n",
        "- Lin et al. Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval, TACL, 2023\n",
        "- etc ...\n",
        "\n",
        "\n",
        "\n",
        "### その他検索実験系ツール\n",
        "- Pyserini https://github.com/castorini/pyserini/\n",
        "  - 検索ツールキットで，「文の実験の再現を２クリックでできる」というように、簡単に論文の結果を再現するというところに焦点を当てています．\n",
        "  - 事前にエンコードされたインデックスが多数ホストされており、手軽に実験を始めることができます。\n",
        "  - 実装されている検索モデルの数が多く、exampleも充実しています。\n",
        "  - 今回紹介するツールの中では最もgithubスター数が多いです。\n",
        "- Pyterrier https://github.com/terrier-org/pyterrier\n",
        "  - 拡張性・柔軟性が高い情報検索フレームワークです。\n",
        "  - パイプライン機能が特徴的で、宣言的に検索実験のコードが書けるようになっています。\n",
        "  - 今回のチュートリアルで紹介されたような検索モデルも一部pluginとして利用できるようになっています。\n",
        "  - ちょっと捻ったような検索実験をするときは、フレームワークとしてとても有用だと思います。\n",
        "    - パイプラインの拡張性が高くなるように設計されているため、新しい検索モデルも統合しやすいです。昔からあるモデルはpyterrierに組み込まれているため、pyterrierだけで新旧の検索モデルを包括的に実験ができます。\n",
        "  - 後述するir-datasetsとの相性も良いです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIbBr8PH8tzQ",
        "outputId": "f3f2a8b6-015e-4eee-be38-8788d254af07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[1minfo: \u001b[mprofile set to 'default'\n",
            "\u001b[1minfo: \u001b[mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[1minfo: \u001b[msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[1minfo: \u001b[mlatest update on 2024-02-08, rust version 1.76.0 (07dca489a 2024-02-04)\n",
            "\u001b[1minfo: \u001b[mdownloading component 'cargo'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'clippy'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rust-docs'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rust-std'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rustc'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rustfmt'\n",
            "\u001b[1minfo: \u001b[minstalling component 'cargo'\n",
            "\u001b[1minfo: \u001b[minstalling component 'clippy'\n",
            "\u001b[1minfo: \u001b[minstalling component 'rust-docs'\n",
            " 14.7 MiB /  14.7 MiB (100 %)   1.3 MiB/s in 10s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'rust-std'\n",
            " 23.9 MiB /  23.9 MiB (100 %)  10.0 MiB/s in  2s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'rustc'\n",
            " 62.3 MiB /  62.3 MiB (100 %)   9.6 MiB/s in  6s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'rustfmt'\n",
            "\u001b[1minfo: \u001b[mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[m - rustc 1.76.0 (07dca489a 2024-02-04)\n",
            "\n",
            "\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[1mPATH\u001b[m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, run:\n",
            "source \"$HOME/.cargo/env\"\n"
          ]
        }
      ],
      "source": [
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZK_9enFr5tO",
        "outputId": "b8ed0a8b-b78c-47dc-b27b-65c523edc05d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/texttron/tevatron\n",
            "  Cloning https://github.com/texttron/tevatron to /tmp/pip-req-build-mla0j9xw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/texttron/tevatron /tmp/pip-req-build-mla0j9xw\n",
            "  Resolved https://github.com/texttron/tevatron to commit 2e5d00ee21d5a7db0bd2ea1463c9150a572106d4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from tevatron==0.0.1) (4.38.1)\n",
            "Collecting datasets>=1.1.3 (from tevatron==0.0.1)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.1.3->tevatron==0.0.1)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=1.1.3->tevatron==0.0.1)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->tevatron==0.0.1) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->tevatron==0.0.1) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->tevatron==0.0.1) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets>=1.1.3->tevatron==0.0.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.1.3->tevatron==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.1.3->tevatron==0.0.1) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.1.3->tevatron==0.0.1) (1.16.0)\n",
            "Building wheels for collected packages: tevatron\n",
            "  Building wheel for tevatron (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tevatron: filename=tevatron-0.0.1-py3-none-any.whl size=38500 sha256=1723654e70eeca3665d2d3fc85f24ac95d3adc1fb064fa0d96629de87ca78120\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-43t9qihd/wheels/bc/1e/c8/3bc95e9e1ca1ebe94b78ca794b9a19de36adf4c6faf86b2346\n",
            "Successfully built tevatron\n",
            "Installing collected packages: dill, multiprocess, datasets, tevatron\n",
            "Successfully installed datasets-2.17.1 dill-0.3.8 multiprocess-0.70.16 tevatron-0.0.1\n",
            "Collecting git+https://github.com/luyug/GradCache\n",
            "  Cloning https://github.com/luyug/GradCache to /tmp/pip-req-build-3micz9i2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/luyug/GradCache /tmp/pip-req-build-3micz9i2\n",
            "  Resolved https://github.com/luyug/GradCache to commit 0c33638cb27c2519ad09c476824d550589a8ec38\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GradCache\n",
            "  Building wheel for GradCache (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GradCache: filename=GradCache-0.1.0-py3-none-any.whl size=14187 sha256=5a41f0c778310d3068cf4dde718354828920de5650ab64feaf12216786aa314e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b378teco/wheels/8e/01/6a/628d30d15cc7970307cd2da576c17315883121bedaad3cf25d\n",
            "Successfully built GradCache\n",
            "Installing collected packages: GradCache\n",
            "Successfully installed GradCache-0.1.0\n",
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m416.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.10.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0+cu113\n",
            "Collecting faiss-cpu==1.7.2\n",
            "  Downloading faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.16.0\n",
            "  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.17.0\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.3/306.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.16.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (4.66.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (14.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.17.0) (2023.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==1.17.0) (1.16.0)\n",
            "Installing collected packages: faiss-cpu, sacremoses, transformers, datasets\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.17.1\n",
            "    Uninstalling datasets-2.17.1:\n",
            "      Successfully uninstalled datasets-2.17.1\n",
            "Successfully installed datasets-1.17.0 faiss-cpu-1.7.2 sacremoses-0.1.1 transformers-4.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/texttron/tevatron\n",
        "!pip install git+https://github.com/luyug/GradCache\n",
        "!pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!PATH=\"$HOME/.cargo/bin:$PATH\" pip install -U faiss-cpu==1.7.2 transformers==4.16.0 datasets==1.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7HiC3mc7f3z"
      },
      "source": [
        "こちらは評価に使うパッケージと、評価スクリプトのコードを使いたいので、Tevatronのソースコードをcloneします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e7psq67F7ei6",
        "outputId": "04ae30c0-6912-4608-8b71-a02c99229714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyserini\n",
            "  Downloading pyserini-0.24.0-py3-none-any.whl (142.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.5.3)\n",
            "Collecting pyjnius>=1.4.0 (from pyserini)\n",
            "  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.16.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.10/dist-packages (from pyserini) (0.1.99)\n",
            "Collecting nmslib>=2.1.1 (from pyserini)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.1.0)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.7.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyserini) (6.0.1)\n",
            "Collecting openai>=1.0.0 (from pyserini)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken>=0.4.0 (from pyserini)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11<2.6.2 (from nmslib>=2.1.1->pyserini)\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini) (5.9.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.12)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->pyserini) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.0.0->pyserini)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (4.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.31.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->pyserini) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.20.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.15.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->pyserini) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->pyserini) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->pyserini)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->pyserini) (2023.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.0->pyserini) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2.1->pyserini) (0.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13547972 sha256=5786c2df2a9cbc3d21fb385f877c23fc5bce2c38e112d63511e209a1ad76f214\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built nmslib\n",
            "Installing collected packages: pyjnius, pybind11, humanfriendly, h11, tiktoken, nmslib, httpcore, coloredlogs, onnxruntime, httpx, openai, pyserini\n",
            "Successfully installed coloredlogs-15.0.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 humanfriendly-10.0 nmslib-2.1.1 onnxruntime-1.17.1 openai-1.13.3 pybind11-2.6.1 pyjnius-1.6.1 pyserini-0.24.0 tiktoken-0.6.0\n",
            "Cloning into 'tevatron'...\n",
            "remote: Enumerating objects: 1534, done.\u001b[K\n",
            "remote: Counting objects: 100% (549/549), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 1534 (delta 384), reused 382 (delta 312), pack-reused 985\u001b[K\n",
            "Receiving objects: 100% (1534/1534), 20.73 MiB | 21.27 MiB/s, done.\n",
            "Resolving deltas: 100% (851/851), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyserini\n",
        "!git clone https://github.com/texttron/tevatron.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M0VfI9pkyhH"
      },
      "source": [
        "## DPRの現時点の検索性能\n",
        "\n",
        "今回はmsmarco-passageでfine-tuningされたDPRを、nfcorpusでファインチューニングしていきます。\n",
        "\n",
        "その前に、今の時点ではどのくらいの性能が出ているのか見てみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTkJFPylDJq"
      },
      "source": [
        "### Tevatronのスクリプトを用いて、nfcorpusで評価\n",
        "\n",
        "nfcorpusはBEIRというベンチマークに含まれているデータセットの一つです。\n",
        "\n",
        "Tevatronには検索モデルをBEIRで簡単に評価できるようなスクリプトがあるので使わせてもらいましょう。\n",
        "BEIRの論文ではRecall@100とnDCG@10で評価しているため、このスクリプトもそれに倣っています。\n",
        "そのため、今回用いる評価指標もRecall@100、nDCG@10です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3SvKbzz2q_8G",
        "outputId": "54dce4df-c018-46c8-9abf-4f4dd675d325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: BASE_IR_MODEL=k-ush/tevatron_dpr\n"
          ]
        }
      ],
      "source": [
        "%env BASE_IR_MODEL=k-ush/tevatron_dpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LNKrM6zok7ET",
        "outputId": "077ddc24-99c9-4196-b780-66435128cc60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-01 17:28:22.817887: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:28:22.817931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:28:22.819366: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:28:22.826612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:28:24.232177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading: 100% 736/736 [00:00<00:00, 4.03MB/s]\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 193kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 2.39MB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 1.76MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 927kB/s]\n",
            "03/01/2024 17:28:33 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:28:33 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "Downloading: 100% 418M/418M [00:06<00:00, 68.7MB/s]\n",
            "Downloading: 100% 3.69k/3.69k [00:00<00:00, 14.5MB/s]\n",
            "Downloading and preparing dataset beir_corpus/nfcorpus to /root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a...\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading: 100% 1.94M/1.94M [00:00<00:00, 25.6MB/s]\n",
            "100% 1/1 [00:00<00:00,  1.52it/s]\n",
            "100% 1/1 [00:00<00:00, 15.07it/s]\n",
            "Dataset beir_corpus downloaded and prepared to /root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 141.04it/s]\n",
            "03/01/2024 17:28:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7f2876793c70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 478.90ex/s]\n",
            "\n",
            "Running tokenization #1:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  24% 9/38 [00:00<00:00, 89.74ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  47% 18/38 [00:00<00:00, 174.30ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  39% 15/38 [00:00<00:00, 139.35ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  68% 26/38 [00:00<00:00, 132.22ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 187.54ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 179.30ex/s]\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 113.70ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  37% 14/38 [00:00<00:00, 129.99ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  26% 10/38 [00:00<00:00, 96.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 166.31ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  24% 9/38 [00:00<00:00, 80.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 153.80ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  24% 9/38 [00:00<00:00, 88.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  47% 18/38 [00:00<00:00, 82.91ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  95% 36/38 [00:00<00:00, 189.00ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 179.97ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  82% 31/38 [00:00<00:00, 99.23ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 102.67ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  68% 26/38 [00:00<00:00, 128.33ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  78% 29/37 [00:00<00:00, 283.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 273.78ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 136.29ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 38/38 [00:00<00:00, 224.91ex/s]\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 155.88ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.46it/s]\n",
            "2024-03-01 17:29:00.121118: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:29:00.121164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:29:00.122372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:29:00.129705: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:29:01.143339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:29:04 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:29:04 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:29:08 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 498.43it/s]\n",
            "03/01/2024 17:29:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7da410299000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  76% 29/38 [00:00<00:00, 283.65ex/s]\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 255.39ex/s]\n",
            "\n",
            "Running tokenization #1:  50% 19/38 [00:00<00:00, 184.86ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  34% 13/38 [00:00<00:00, 125.45ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 176.89ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  47% 18/38 [00:00<00:00, 145.14ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  76% 29/38 [00:00<00:00, 139.53ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 149.16ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  87% 33/38 [00:00<00:00, 133.01ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  32% 12/38 [00:00<00:00, 103.41ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 126.43ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  32% 12/38 [00:00<00:00, 113.78ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  24% 9/38 [00:00<00:00, 85.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  87% 33/38 [00:00<00:00, 107.75ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 109.26ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  63% 24/38 [00:00<00:00, 113.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 118.37ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  24% 9/38 [00:00<00:00, 70.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  24% 9/38 [00:00<00:00, 86.51ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  76% 29/38 [00:00<00:00, 93.28ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  58% 22/38 [00:00<00:00, 99.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 105.42ex/s]\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 98.30ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 123.83ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  71% 27/38 [00:00<00:00, 131.82ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  30% 11/37 [00:00<00:00, 108.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  54% 20/37 [00:00<00:00, 171.84ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 113.45ex/s]\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 160.20ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 146.09ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.49it/s]\n",
            "2024-03-01 17:29:22.274248: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:29:22.274295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:29:22.275613: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:29:22.282398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:29:23.286867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:29:27 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:29:27 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:29:29 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 611.68it/s]\n",
            "03/01/2024 17:29:29 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7857c06f51b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  76% 29/38 [00:00<00:00, 280.55ex/s]\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 218.40ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  29% 11/38 [00:00<00:00, 96.35ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1:  68% 26/38 [00:00<00:00, 111.52ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  21% 8/38 [00:00<00:00, 72.09ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  55% 21/38 [00:00<00:00, 84.80ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 111.79ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  18% 7/38 [00:00<00:00, 69.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  47% 18/38 [00:00<00:00, 78.82ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  79% 30/38 [00:00<00:00, 82.93ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  37% 14/38 [00:00<00:00, 68.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  21% 8/38 [00:00<00:00, 58.36ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 76.96ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  55% 21/38 [00:00<00:00, 68.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  39% 15/38 [00:00<00:00, 61.86ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  13% 5/38 [00:00<00:00, 48.88ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  89% 34/38 [00:00<00:00, 62.70ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  29% 11/38 [00:00<00:00, 49.81ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  58% 22/38 [00:00<00:00, 58.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 63.56ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  13% 5/38 [00:00<00:00, 47.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  76% 29/38 [00:00<00:00, 59.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  32% 12/38 [00:00<00:00, 57.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  50% 19/38 [00:00<00:00, 54.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  89% 34/38 [00:00<00:00, 50.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   8% 3/38 [00:00<00:01, 29.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 56.79ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  92% 35/38 [00:00<00:00, 57.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  47% 18/38 [00:00<00:00, 57.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  68% 26/38 [00:00<00:00, 57.32ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 56.46ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  13% 5/38 [00:00<00:00, 47.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  63% 24/38 [00:00<00:00, 54.80ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  87% 33/38 [00:00<00:00, 60.28ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  47% 18/38 [00:00<00:00, 57.88ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 58.00ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  79% 30/38 [00:00<00:00, 56.17ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  14% 5/37 [00:00<00:00, 48.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  74% 28/38 [00:00<00:00, 68.41ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  61% 23/38 [00:00<00:00, 70.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 57.46ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  19% 7/37 [00:00<00:00, 64.75ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  38% 14/37 [00:00<00:00, 66.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  95% 36/38 [00:00<00:00, 70.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 64.85ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  43% 16/37 [00:00<00:00, 75.91ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 68.89ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 83.77ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 88.96ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.50it/s]\n",
            "2024-03-01 17:29:45.387105: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:29:45.387162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:29:45.388568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:29:45.395489: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:29:46.417387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:29:50 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:29:50 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:29:52 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 676.72it/s]\n",
            "03/01/2024 17:29:52 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7a765819d1b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 423.42ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  34% 13/38 [00:00<00:00, 129.29ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  55% 21/38 [00:00<00:00, 198.64ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  68% 26/38 [00:00<00:00, 126.33ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  47% 18/38 [00:00<00:00, 176.99ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 174.51ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 121.55ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  95% 36/38 [00:00<00:00, 142.36ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 131.44ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 195.75ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  16% 6/38 [00:00<00:00, 55.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 99.60ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  45% 17/38 [00:00<00:00, 86.49ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  24% 9/38 [00:00<00:00, 87.64ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  24% 9/38 [00:00<00:00, 86.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  79% 30/38 [00:00<00:00, 104.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  55% 21/38 [00:00<00:00, 100.16ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  55% 21/38 [00:00<00:00, 103.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  29% 11/38 [00:00<00:00, 105.35ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 98.20ex/s] \n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 124.82ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  46% 17/37 [00:00<00:00, 161.72ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 114.67ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  76% 29/38 [00:00<00:00, 145.75ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  46% 17/37 [00:00<00:00, 148.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 178.71ex/s]\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 123.47ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 156.87ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.49it/s]\n",
            "2024-03-01 17:30:07.688610: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:30:07.688667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:30:07.690443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:30:07.701074: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:30:09.189403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:30:13 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:30:13 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:30:15 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 650.58it/s]\n",
            "03/01/2024 17:30:15 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7898481c1000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 491.10ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  42% 16/38 [00:00<00:00, 149.72ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  34% 13/38 [00:00<00:00, 121.95ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  29% 11/38 [00:00<00:00, 105.98ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 163.49ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  18% 7/38 [00:00<00:00, 68.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  74% 28/38 [00:00<00:00, 136.17ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 160.11ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  58% 22/38 [00:00<00:00, 89.25ex/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 183.68ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  26% 10/38 [00:00<00:00, 95.44ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  89% 34/38 [00:00<00:00, 99.13ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 99.28ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 97.06ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  61% 23/38 [00:00<00:00, 110.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  34% 13/38 [00:00<00:00, 114.53ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  74% 28/38 [00:00<00:00, 129.93ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 114.32ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  32% 12/38 [00:00<00:00, 117.35ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 131.11ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 144.15ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  79% 30/38 [00:00<00:00, 150.21ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 131.31ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 230.81ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 142.34ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.49it/s]\n",
            "2024-03-01 17:30:30.228478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:30:30.228529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:30:30.229851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:30:30.236898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:30:31.245009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:30:35 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:30:35 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:30:38 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 698.82it/s]\n",
            "03/01/2024 17:30:38 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7ae9103dd000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  79% 30/38 [00:00<00:00, 281.06ex/s]\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 298.79ex/s]\n",
            "\n",
            "Running tokenization #1:  39% 15/38 [00:00<00:00, 147.93ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  21% 8/38 [00:00<00:00, 79.35ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  79% 30/38 [00:00<00:00, 131.12ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  29% 11/38 [00:00<00:00, 90.05ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  55% 21/38 [00:00<00:00, 107.40ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 122.60ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  29% 11/38 [00:00<00:00, 108.22ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  29% 11/38 [00:00<00:00, 106.70ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  58% 22/38 [00:00<00:00, 99.47ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  92% 35/38 [00:00<00:00, 116.71ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 107.61ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  63% 24/38 [00:00<00:00, 120.30ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  21% 8/38 [00:00<00:00, 69.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  87% 33/38 [00:00<00:00, 95.80ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  58% 22/38 [00:00<00:00, 79.06ex/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 130.69ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 99.40ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  45% 17/38 [00:00<00:00, 76.03ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  82% 31/38 [00:00<00:00, 80.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  45% 17/38 [00:00<00:00, 82.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  26% 10/38 [00:00<00:00, 98.26ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 87.94ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  34% 13/38 [00:00<00:00, 128.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  55% 21/38 [00:00<00:00, 103.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 91.66ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  57% 21/37 [00:00<00:00, 193.51ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 94.01ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 127.15ex/s]\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 145.52ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 150.99ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 178.23ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.45it/s]\n",
            "2024-03-01 17:30:52.651370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:30:52.651424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:30:52.652725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:30:52.659731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:30:53.702218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:30:58 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:30:58 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:31:00 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 643.00it/s]\n",
            "03/01/2024 17:31:00 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7a8b40141000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  87% 33/38 [00:00<00:00, 314.92ex/s]\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 290.69ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  58% 22/38 [00:00<00:00, 183.91ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  63% 24/38 [00:00<00:00, 237.38ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 198.91ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 139.79ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 153.77ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  45% 17/38 [00:00<00:00, 167.87ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 190.85ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 179.17ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  26% 10/38 [00:00<00:00, 84.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  29% 11/38 [00:00<00:00, 101.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  63% 24/38 [00:00<00:00, 109.68ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  58% 22/38 [00:00<00:00, 213.86ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 121.21ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 199.77ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  59% 22/37 [00:00<00:00, 212.43ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 137.11ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 173.02ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 150.34ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 176.27ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.45it/s]\n",
            "2024-03-01 17:31:14.772611: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:31:14.772659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:31:14.774005: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:31:14.781200: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:31:15.786016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:31:19 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:31:19 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 17:31:22 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 511.50it/s]\n",
            "03/01/2024 17:31:22 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7b97e80ed1b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #1:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 305.98ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  55% 21/38 [00:00<00:00, 205.20ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 182.93ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:  26% 10/38 [00:00<00:00, 86.93ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  68% 26/38 [00:00<00:00, 112.61ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  26% 10/38 [00:00<00:00, 91.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 135.48ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  55% 21/38 [00:00<00:00, 96.81ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  29% 11/38 [00:00<00:00, 89.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  47% 18/38 [00:00<00:00, 179.96ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  87% 33/38 [00:00<00:00, 102.83ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 101.38ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  74% 28/38 [00:00<00:00, 131.05ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  82% 31/38 [00:00<00:00, 92.22ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 159.39ex/s]\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 134.54ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 96.20ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  34% 13/38 [00:00<00:00, 129.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  53% 20/38 [00:00<00:00, 96.23ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  68% 26/38 [00:00<00:00, 119.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  26% 10/38 [00:00<00:00, 94.70ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 136.06ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 116.06ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  49% 18/37 [00:00<00:00, 173.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  68% 26/38 [00:00<00:00, 132.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 146.80ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 175.48ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 189.11ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.47it/s]\n",
            "2024-03-01 17:31:36.896519: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 17:31:36.896569: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 17:31:36.897861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 17:31:36.905073: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 17:31:37.902872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 17:31:41 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 17:31:41 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "Downloading: 100% 3.61k/3.61k [00:00<00:00, 16.8MB/s]\n",
            "Downloading and preparing dataset beir/nfcorpus to /root/.cache/huggingface/datasets/Tevatron___beir/nfcorpus/1.0.0/bbd662c3d4cd99695bdc9c251b14cb18c0fcc7cb571d8373c5746b97789a4b9a...\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading: 100% 19.9k/19.9k [00:00<00:00, 31.3MB/s]\n",
            "100% 1/1 [00:00<00:00,  1.49it/s]\n",
            "100% 1/1 [00:00<00:00, 1206.65it/s]\n",
            "Dataset beir downloaded and prepared to /root/.cache/huggingface/datasets/Tevatron___beir/nfcorpus/1.0.0/bbd662c3d4cd99695bdc9c251b14cb18c0fcc7cb571d8373c5746b97789a4b9a. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 718.69it/s]\n",
            "03/01/2024 17:31:45 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7a95dc3bfb50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0: 100% 27/27 [00:00<00:00, 739.72ex/s]\n",
            "\n",
            "Running tokenization #1: 100% 27/27 [00:00<00:00, 491.91ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 27/27 [00:00<00:00, 550.56ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/27 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 27/27 [00:00<00:00, 1712.40ex/s]\n",
            "Running tokenization #3: 100% 27/27 [00:00<00:00, 685.88ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 27/27 [00:00<00:00, 805.23ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 27/27 [00:00<00:00, 826.34ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 27/27 [00:00<00:00, 561.44ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 27/27 [00:00<00:00, 969.95ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 27/27 [00:00<00:00, 754.79ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 27/27 [00:00<00:00, 1295.63ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 26/26 [00:00<00:00, 2704.53ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 6/6 [00:03<00:00,  1.58it/s]\n",
            "03/01/2024 17:31:57 - INFO - __main__ -   Pattern match found 8 files; loading them into index.\n",
            "Loading shards into index: 100% 8/8 [00:00<00:00, 382.04it/s]\n",
            "03/01/2024 17:31:57 - INFO - __main__ -   Index Search Start\n",
            "100% 6/6 [00:00<00:00, 58.33it/s]\n",
            "03/01/2024 17:31:57 - INFO - __main__ -   Index Search Finished\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "jtreceval-0.0.5-jar-with-dependencies.jar: 1.79MB [00:00, 4.76MB/s]                \n",
            "Downloading qrels from https://raw.githubusercontent.com/castorini/anserini-tools/master/topics-and-qrels/qrels.beir-v1.0.0-nfcorpus.test.txt\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-mrecall.100', '-mndcg_cut.10', '/root/.cache/anserini/topics-and-qrels/qrels.beir-v1.0.0-nfcorpus.test.txt', 'beir_embedding_k-ush/tevatron_dpr/rank.nfcorpus.trec']\n",
            "Results:\n",
            "recall_100            \tall\t0.1747\n",
            "ndcg_cut_10           \tall\t0.1620\n"
          ]
        }
      ],
      "source": [
        "!mkdir beir_embedding_k-ush\n",
        "!bash tevatron/scripts/eval_beir.sh $BASE_IR_MODEL nfcorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "評価が終わりました！\n",
        "\n",
        "評価結果を見やすく表形式で表示してみましょう。"
      ],
      "metadata": {
        "id": "APluCkvNmnFO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RW6tyrS4KcsM",
        "outputId": "e5bc4daf-dc72-43a7-b888-031b957e5274"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008980256121069147,\n        \"min\": 0.162,\n        \"max\": 0.1747,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.162,\n          0.1747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-00b41719-a610-4191-b9fe-455547c8e1a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>R@100</th>\n",
              "      <td>0.1747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nDCG@10</th>\n",
              "      <td>0.1620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00b41719-a610-4191-b9fe-455547c8e1a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00b41719-a610-4191-b9fe-455547c8e1a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00b41719-a610-4191-b9fe-455547c8e1a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eb900fe1-4328-4754-a657-56086d3c6324\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb900fe1-4328-4754-a657-56086d3c6324')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eb900fe1-4328-4754-a657-56086d3c6324 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "              0\n",
              "R@100    0.1747\n",
              "nDCG@10  0.1620"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "\n",
        "def read_eval_result(result_filename: str) -> Dict[str, float]:\n",
        "  recall_100 = None\n",
        "  ndcg_cut_10 = None\n",
        "  with open(result_filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if \"recall_100\" in line:\n",
        "          recall_100 = float(line.split(\"\\t\")[-1])\n",
        "\n",
        "      if \"ndcg_cut_10\" in line:\n",
        "          ndcg_cut_10 = float(line.split(\"\\t\")[-1])\n",
        "  return {\n",
        "      \"Recall@100\": recall_100,\n",
        "      \"nDCG@10\": ndcg_cut_10,\n",
        "  }\n",
        "\n",
        "!mkdir -p eval_results\n",
        "!python -m pyserini.eval.trec_eval -c -mrecall.100 -mndcg_cut.10 beir-v1.0.0-nfcorpus-test beir_embedding_k-ush/tevatron_dpr/rank.nfcorpus.trec > eval_results/dpr.result\n",
        "\n",
        "before_trian_result = read_eval_result(\"eval_results/dpr.result\")\n",
        "pd.DataFrame([before_trian_result]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb5XKxDh6w6l"
      },
      "source": [
        "## DPRを訓練する\n",
        "\n",
        "それでは、DPRを訓練していきます。\n",
        "\n",
        "引き続きTevatronを使って、nfcorpusの訓練データセットでDPRを訓練していきますが、その前にnfcorpusの訓練データをTevatronで扱えるフォーマットに直す必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Utl_tyrCLIe"
      },
      "source": [
        "### 訓練データの整形\n",
        "Tevatronでの訓練データセットの形は以下の形式のjsonで構成されるjsonlです。\n",
        "\n",
        "内部的にはdatasetsのload_datasetを読んでいるので、load_datasetで読んで下記の構造のデータになればなんでも良いです。\n",
        "\n",
        "```\n",
        "{\n",
        "  query_id: ...,\n",
        "  query: ...,\n",
        "  positive_passages: [\n",
        "    {\n",
        "      docid: ...,\n",
        "      title: ...,\n",
        "      text: ...,\n",
        "    }\n",
        "  ],\n",
        "  negative_passages: [\n",
        "    {\n",
        "      docid: ...,\n",
        "      title: ...,\n",
        "      text: ...,\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### ir-datasets https://github.com/allenai/ir_datasets/\n",
        "\n",
        "nfcorpusの訓練データを整形するわけですが、\n",
        "その訓練データは ir-datasetsから引っ張ってきます。\n",
        "ir-datasetsは情報検索向けのデータセットを集めて、同じようなインターフェースからアクセスできるようにしているライブラリです。\n",
        "\n",
        "以下のような特徴があります。\n",
        "\n",
        "- 多くの情報検索データセットを網羅\n",
        "- 沢山のデータセットを共通化されたインターフェースからアクセスできる\n",
        "- pip installだけで使い始めることができる\n",
        "- 膨大なコーパスから高速な文書のlookupができるDocStoreなど、欲しい機能が揃っている"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-pi0qXwDcV7"
      },
      "outputs": [],
      "source": [
        "!pip install ir_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Htu3DtdDDRT1"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Callable\n",
        "from pathlib import Path\n",
        "from typing import Union, Dict, List, Set, Callable\n",
        "from collections import defaultdict, namedtuple\n",
        "import random\n",
        "import json\n",
        "\n",
        "import ir_datasets as irds\n",
        "from tqdm import tqdm\n",
        "\n",
        "Qrels = Dict[str, Dict[str, str]]\n",
        "\n",
        "class IrdsToTevatronDataset(object):\n",
        "  def __init__(self, dataset_key: str) -> None:\n",
        "    self.dataset = self._load_dataset(dataset_key)\n",
        "\n",
        "  def _load_dataset(self, dataset_key: str) -> irds.Dataset:\n",
        "    return irds.load(dataset_key)\n",
        "\n",
        "  def load_doc_ids(self) -> List[str]:\n",
        "    doc_ids = []\n",
        "    for doc in tqdm(self.dataset.docs_iter(), total=self.dataset.docs_count(), desc=\"loading doc id\"):\n",
        "      doc_ids.append(doc.doc_id)\n",
        "    return doc_ids\n",
        "\n",
        "  def load_query_table(self, query_field: str = \"text\") -> Dict[str, str]:\n",
        "    queries = {}\n",
        "    for query in tqdm(self.dataset.queries_iter(), total=self.dataset.queries_count(), desc=\"loading query\"):\n",
        "      queries[query.query_id] = getattr(query, query_field)\n",
        "    return queries\n",
        "\n",
        "  def load_qrels_table(self) -> Qrels:\n",
        "    qrels = defaultdict(dict)\n",
        "    for qrel in tqdm(self.dataset.qrels_iter(), total=self.dataset.qrels_count(), desc=\"loading qrels\"):\n",
        "      qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "    return qrels\n",
        "\n",
        "  def sample_random_negatives(self, doc_ids: List[str], exclude_doc_ids: Union[Set[str], List[str]], k: int = 1) -> List[str]:\n",
        "    exclude_doc_ids = set(exclude_doc_ids)\n",
        "    sample_source = [doc_id for doc_id in doc_ids if not doc_id in exclude_doc_ids]\n",
        "    negative_doc_ids = random.choices(sample_source, k=k)\n",
        "    return random.choices(sample_source, k=k)\n",
        "\n",
        "  def prepare_train_dataset(self, output_path: Union[str, Path], doc_preprocess: Callable[[namedtuple], str], queries_num: int = 500):\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    docstore = self.dataset.docs_store()\n",
        "    doc_ids = self.load_doc_ids()\n",
        "    queries = self.load_query_table()\n",
        "    qrels = self.load_qrels_table()\n",
        "\n",
        "    with output_path.open(\"w\") as fw:\n",
        "      total = min(len(queries), queries_num)\n",
        "      for i, qid in enumerate(tqdm(queries, desc=\"writing train dataset\", total=total)):\n",
        "        if i >= total: break\n",
        "\n",
        "        query = queries[qid]\n",
        "        relevant_doc_ids = [doc_id for doc_id in qrels[qid].keys() if qrels[qid][doc_id] > 0]\n",
        "        negative_ids = self.sample_random_negatives(doc_ids, relevant_doc_ids, k=len(relevant_doc_ids))\n",
        "        negatives = [doc_preprocess(doc) for docid, doc in docstore.get_many(negative_ids).items()]\n",
        "        positives = [doc_preprocess(doc) for docid, doc in docstore.get_many(relevant_doc_ids).items()]\n",
        "        train_json = {\n",
        "            \"query_id\": qid,\n",
        "            \"query\": query,\n",
        "            \"positive_passages\": positives,\n",
        "            \"negative_passages\": negatives\n",
        "        }\n",
        "        fw.write(json.dumps(train_json, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "dataset_key = \"beir/nfcorpus/train\"\n",
        "dataset_path = \"/content/nfcorpus.train.jsonl\"\n",
        "queries_num = 1000\n",
        "\n",
        "def nfcorpus_doc_preprocess(doc: namedtuple) -> str:\n",
        "  return {\n",
        "      \"docid\": doc.doc_id,\n",
        "      \"title\": doc.title,\n",
        "      \"text\": doc.text,\n",
        "  }\n",
        "\n",
        "irds_to_tev = IrdsToTevatronDataset(dataset_key)\n",
        "irds_to_tev.prepare_train_dataset(dataset_path, doc_preprocess=nfcorpus_doc_preprocess, queries_num=queries_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvLfJW4DDRkf"
      },
      "source": [
        "### **訓練の実行**\n",
        "\n",
        "それでは先ほど評価したDPRを、nfcorpusの訓練データで訓練していきます。\n",
        "\n",
        "訓練には`tevatron.driver.train`モジュールを用います。\n",
        "内部的には、transformersのTrainerを使っており、カスタマイズもしやすくなっています。\n",
        "\n",
        "今回は単純なDPRの訓練ですが、Tevatronではcross-encoderを教師とした蒸留や、spladeやunicoilなどの教師あり疎検索モデルの訓練も可能です。\n",
        "\n",
        "主なパラメータの説明\n",
        "\n",
        "| 名前 | 説明 |\n",
        "| - | - |\n",
        "| output_dir | モデルの出力先ディレクトリ |\n",
        "| model_name_or_path | 学習対照のモデル |\n",
        "| dataset_name | データセット、もしくはそのフォーマットの指定 |\n",
        "| train_n_passages | 1 instanceの訓練で使うパッセージの数 |\n",
        "| grad_cache | GradCacheというcross-batchに対照学習を行うライブラリを使うかどうか |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 大規模言語モデルの訓練設定\n",
        "大規模言語モデルに基づく検索モデルを訓練するときの方法やテクニックとしてよく用いられるものには以下のようなものがあります。\n",
        "- 蒸留\n",
        "  - cross encoderは密検索モデルや疎検索モデルより性能が高いですが、計算コストが大きいという課題がありました。\n",
        "  - cross encoderの性能の高さを活かしつつ推論時の計算コストを小さくするために、cross encoderを教師として密検索モデルや疎検索モデルを訓練すると、性能が向上するということが知られています。\n",
        "- 負例選択（negatives minitng）\n",
        "  - in-batch negatives\n",
        "    - 先ほどDPRの説明のところで紹介があった手法ですが、広くよく使われる設定です。\n",
        "  - ANCE negatives\n",
        "    - 学習中のモデルから負例を得る方法はhard negativesと呼ばれます。\n",
        "    - やり方にもバリエーションがあり、ANCEのように学習が進むごとにnegativesを更新していく方法や、1度 in-batch negatives等で学習したモデルから負例を得て、得られた負例を使ってモデルをさらに学習させる等があります。\n",
        "      - Zhang et al. Optimizing Dense Retrieval Model Training with Hard Negatives. SIGIR, 2021.\n",
        "  - cross-batch negatives\n",
        "    - in-batch negatives では、バッチサイズが大きくなるほど負例の数も大きくなります。一般に負例の数が大きくなると性能も高くなることが知られているので、in-batch negativesではなるべく譜例を大きくしたいです。\n",
        "    - ですが、GPUメモリが十分に大きくないとバッチサイズは大きくできません。\n",
        "    - これを克服する方法の一つとして、「他のバッチのデータを負例として使う」という方法があります。これがcross-batch negativesです。\n",
        "    - TevatronはGradcacheというライブラリを学習に使っていますが、これはcross-batch negativesに分類されるような、負例選択手法です。\n",
        "      - Gao et al. Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. RepL4NLP, 2021.\n",
        "    - MoCoというcross-batch negatives手法とin-batch negativesでは性能の差が小さいため、多くの譜例を使えるcross-batchの方が有利という論文がある[2]が、32Kくらいバッチサイズを大きくするとin-batch negativesの方が性能が高いという論文[3]もあり、in-batchとcross-batchのどちらが良いかはまだはっきりしているとは言えない。\n",
        "      - [2] Izacard et al. Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning. TMLR, 2022.\n",
        "      - [3] Wang et al. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv, 2022.\n"
      ],
      "metadata": {
        "id": "Iw9JPGhliE6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDTm2mkA7Kyx"
      },
      "outputs": [],
      "source": [
        "%env TRAINED_DPR_DIR=dpr_nfcorpus\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m tevatron.driver.train \\\n",
        "  --output_dir $TRAINED_DPR_DIR \\\n",
        "  --model_name_or_path $BASE_IR_MODEL \\\n",
        "  --save_steps 10 \\\n",
        "  --dataset_name Tevatron/wikipedia-nq \\\n",
        "  --train_dir /content/nfcorpus.train.jsonl \\\n",
        "  --fp16 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --positive_passage_no_shuffle \\\n",
        "  --train_n_passages 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --q_max_len 32 \\\n",
        "  --p_max_len 156 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --logging_steps 500 \\\n",
        "  --grad_cache \\\n",
        "  --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHfZw1iBIYdV"
      },
      "source": [
        "## 評価\n",
        "\n",
        "それでは、訓練したDPRを評価してみましょう！\n",
        "\n",
        "[最初に評価した時](#scrollTo=5M0VfI9pkyhH)と同じ nfcorpus のテストデータで評価します。\n",
        "手順も同じです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CORqroFAIX_j"
      },
      "outputs": [],
      "source": [
        "!bash tevatron/scripts/eval_beir.sh dpr_nfcorpus nfcorpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq1XT_39-Xu6"
      },
      "outputs": [],
      "source": [
        "!python -m pyserini.eval.trec_eval -c -mrecall.100 -mndcg_cut.10 beir-v1.0.0-nfcorpus-test beir_embedding_${TRAINED_DPR_DIR}/rank.nfcorpus.trec > eval_results/${TRAINED_DPR_DIR}.result\n",
        "trained_dpr_result = read_eval_result(\"eval_results/dpr_nfcorpus.result\")\n",
        "pd.DataFrame([trained_dpr_result]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に、訓練前と後を同じ表にまとめて性能の変化を確認し、終了です。"
      ],
      "metadata": {
        "id": "uuQaVQVuI0pG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxCZ5KWiYfjB"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([before_trian_result, trained_dpr_result]).T"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}