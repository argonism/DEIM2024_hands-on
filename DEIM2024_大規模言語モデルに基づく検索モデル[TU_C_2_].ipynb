{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argonism/DEIM2024_hands-on/blob/main/DEIM2024_%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%86%E3%82%99%E3%83%AB%E3%81%AB%E5%9F%BA%E3%81%A4%E3%82%99%E3%81%8F%E6%A4%9C%E7%B4%A2%E3%83%A2%E3%83%86%E3%82%99%E3%83%AB%5BTU_C_2_%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozXAprDmskva"
      },
      "source": [
        "#\n",
        "# DEIM 2024 チュートリアル **大規模言語モデルに基づく検索モデル** \\[TU-C-2\\]\n",
        "\n",
        "## 概要\n",
        "近年の大規模言語モデルに基づく検索モデルを用いた検索実験のデモ\n",
        "\n",
        "DPRの学習と評価のデモを通して、よく用いられているフレームワークやツールを示します．\n",
        "\n",
        "ここでは，nfcorpusというデータセットを用いて，DPRというBERTベースの密検索モデルを訓練します．\n",
        "1. ファインチューニングする前に、DPRのnfcorpusでの性能を評価します．\n",
        "1. nfcorpusの訓練セットでDPRを訓練します．\n",
        "1. 2.で得られたDPRを再びnfcorpusで評価し，DPRのin-domainの検索性能を確認します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx8iVg1l5Aa6"
      },
      "source": [
        "## 始める前に\n",
        "**ランタイムのタイプがGPU（e.g. T4 GPU）になっていることを確認してください！**\n",
        "\n",
        "確認方法\n",
        "- colab上部のナビゲーションバーから「ランタイム」> 「ランタイムのタイプを変更」\n",
        "- 「ハードウェア アクセラレータ」の指定がGPUになっていることを確認して、閉じる。\n",
        "- なっていなければ選択できるGPUを指定して「保存」を押す"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdp0cI1ztaOz"
      },
      "source": [
        "## 依存関係のインストール\n",
        "\n",
        "今回は、DPRのファインチューニングにTevatronという大規模言語モデルの訓練ツールキットを使います。\n",
        "\n",
        "そのため、まずはTevatronと、依存パッケージをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4SJP_8DZapP"
      },
      "outputs": [],
      "source": [
        "import time; COLAB_START_TIME = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIbBr8PH8tzQ",
        "outputId": "6bf5b350-a3a7-40df-dcc5-7979d2bbdf11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[1minfo: \u001b[mprofile set to 'default'\n",
            "\u001b[1minfo: \u001b[mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[1minfo: \u001b[msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[1minfo: \u001b[mlatest update on 2024-02-08, rust version 1.76.0 (07dca489a 2024-02-04)\n",
            "\u001b[1minfo: \u001b[mdownloading component 'cargo'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'clippy'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rust-docs'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rust-std'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rustc'\n",
            "\u001b[1minfo: \u001b[mdownloading component 'rustfmt'\n",
            "\u001b[1minfo: \u001b[minstalling component 'cargo'\n",
            "  8.5 MiB /   8.5 MiB (100 %)   6.5 MiB/s in  1s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'clippy'\n",
            "\u001b[1minfo: \u001b[minstalling component 'rust-docs'\n",
            " 14.7 MiB /  14.7 MiB (100 %)   1.0 MiB/s in 11s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'rust-std'\n",
            " 23.9 MiB /  23.9 MiB (100 %)   7.9 MiB/s in  3s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'rustc'\n",
            " 62.3 MiB /  62.3 MiB (100 %)  11.5 MiB/s in  6s ETA:  0s\n",
            "\u001b[1minfo: \u001b[minstalling component 'rustfmt'\n",
            "\u001b[1minfo: \u001b[mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[m - rustc 1.76.0 (07dca489a 2024-02-04)\n",
            "\n",
            "\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[1mPATH\u001b[m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, run:\n",
            "source \"$HOME/.cargo/env\"\n"
          ]
        }
      ],
      "source": [
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-oPgZDg6Eq4"
      },
      "source": [
        "### **Tevatron**\n",
        "\n",
        "大規模言語モデルに基づく検索モデルの学習や評価に焦点を当てたpythonフレームワーク\n",
        "\n",
        "情報検索系のフレームワークの中では大規模言語モデル系の検索モデルの学習に強いという特徴があります．\n",
        "\n",
        "### その他のIRツール\n",
        "- Ancerini・Pyserini\n",
        "  - 検索ツールキットで，簡単に論文の結果を再現するというところに焦点を当てている．PyseriniはAnceriniのpythonバインディング．\n",
        "- Terrier・Pyterrier\n",
        "  - 拡張性・柔軟性が高い情報検索フレームワークで、最近の大規模言語モデルに基づく検索モデルの実装もプラグインとして公開されている。Pyterrierはterrierのpythonバインディング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZK_9enFr5tO",
        "outputId": "ad814afd-2d36-452d-8432-6435495702c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/texttron/tevatron\n",
            "  Cloning https://github.com/texttron/tevatron to /tmp/pip-req-build-ewl7kzz3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/texttron/tevatron /tmp/pip-req-build-ewl7kzz3\n",
            "  Resolved https://github.com/texttron/tevatron to commit 2e5d00ee21d5a7db0bd2ea1463c9150a572106d4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from tevatron==0.0.1) (4.38.1)\n",
            "Collecting datasets>=1.1.3 (from tevatron==0.0.1)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.1.3->tevatron==0.0.1)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=1.1.3->tevatron==0.0.1)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->tevatron==0.0.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->tevatron==0.0.1) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->tevatron==0.0.1) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->tevatron==0.0.1) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->tevatron==0.0.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets>=1.1.3->tevatron==0.0.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->tevatron==0.0.1) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.1.3->tevatron==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.1.3->tevatron==0.0.1) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.1.3->tevatron==0.0.1) (1.16.0)\n",
            "Building wheels for collected packages: tevatron\n",
            "  Building wheel for tevatron (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tevatron: filename=tevatron-0.0.1-py3-none-any.whl size=38500 sha256=317516cfecd9303df4db226444123e461f143a0725d1fdf81792faef8b0e9a55\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-44c0mn1j/wheels/bc/1e/c8/3bc95e9e1ca1ebe94b78ca794b9a19de36adf4c6faf86b2346\n",
            "Successfully built tevatron\n",
            "Installing collected packages: dill, multiprocess, datasets, tevatron\n",
            "Successfully installed datasets-2.17.1 dill-0.3.8 multiprocess-0.70.16 tevatron-0.0.1\n",
            "Collecting git+https://github.com/luyug/GradCache\n",
            "  Cloning https://github.com/luyug/GradCache to /tmp/pip-req-build-x2bqs9gk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/luyug/GradCache /tmp/pip-req-build-x2bqs9gk\n",
            "  Resolved https://github.com/luyug/GradCache to commit 0c33638cb27c2519ad09c476824d550589a8ec38\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GradCache\n",
            "  Building wheel for GradCache (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GradCache: filename=GradCache-0.1.0-py3-none-any.whl size=14187 sha256=620ea1248bf05ca62c9c1081d248fcfe7c5e17570adaa2f643bfeaed111ba8b6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-py_wr4u2/wheels/8e/01/6a/628d30d15cc7970307cd2da576c17315883121bedaad3cf25d\n",
            "Successfully built GradCache\n",
            "Installing collected packages: GradCache\n",
            "Successfully installed GradCache-0.1.0\n",
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.10.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0+cu113\n",
            "Collecting faiss-cpu==1.7.2\n",
            "  Downloading faiss_cpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.16.0\n",
            "  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.17.0\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.3/306.3 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.16.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (4.66.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (14.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.17.0) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.17.0) (2023.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==1.17.0) (1.16.0)\n",
            "Installing collected packages: faiss-cpu, sacremoses, transformers, datasets\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.17.1\n",
            "    Uninstalling datasets-2.17.1:\n",
            "      Successfully uninstalled datasets-2.17.1\n",
            "Successfully installed datasets-1.17.0 faiss-cpu-1.7.2 sacremoses-0.1.1 transformers-4.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/texttron/tevatron\n",
        "!pip install git+https://github.com/luyug/GradCache\n",
        "!pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!PATH=\"$HOME/.cargo/bin:$PATH\" pip install -U faiss-cpu==1.7.2 transformers==4.16.0 datasets==1.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7HiC3mc7f3z"
      },
      "source": [
        "こちらは評価に使うパッケージと、評価スクリプトのコードを使いたいので、Tevatronのソースコードをcloneします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7psq67F7ei6",
        "outputId": "3ca697a8-afb9-48b2-fac4-b956f5ba1a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyserini\n",
            "  Downloading pyserini-0.24.0-py3-none-any.whl (142.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.5.3)\n",
            "Collecting pyjnius>=1.4.0 (from pyserini)\n",
            "  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.16.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.10/dist-packages (from pyserini) (0.1.99)\n",
            "Collecting nmslib>=2.1.1 (from pyserini)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.1.0)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.7.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyserini) (6.0.1)\n",
            "Collecting openai>=1.0.0 (from pyserini)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken>=0.4.0 (from pyserini)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11<2.6.2 (from nmslib>=2.1.1->pyserini)\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini) (5.9.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.12)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->pyserini) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.0.0->pyserini)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (4.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.31.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->pyserini) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.20.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.15.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->pyserini) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->pyserini) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->pyserini)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->pyserini) (2023.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.0->pyserini) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2.1->pyserini) (0.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13578648 sha256=b4a1f537895c0625a2316a3eec699b39c19c22b5ec7e2694530ce00551c9d282\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built nmslib\n",
            "Installing collected packages: pyjnius, pybind11, humanfriendly, h11, tiktoken, nmslib, httpcore, coloredlogs, onnxruntime, httpx, openai, pyserini\n",
            "Successfully installed coloredlogs-15.0.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 humanfriendly-10.0 nmslib-2.1.1 onnxruntime-1.17.1 openai-1.13.3 pybind11-2.6.1 pyjnius-1.6.1 pyserini-0.24.0 tiktoken-0.6.0\n",
            "Cloning into 'tevatron'...\n",
            "remote: Enumerating objects: 1534, done.\u001b[K\n",
            "remote: Counting objects: 100% (549/549), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 1534 (delta 384), reused 382 (delta 312), pack-reused 985\u001b[K\n",
            "Receiving objects: 100% (1534/1534), 20.73 MiB | 13.80 MiB/s, done.\n",
            "Resolving deltas: 100% (851/851), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyserini\n",
        "!git clone https://github.com/texttron/tevatron.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M0VfI9pkyhH"
      },
      "source": [
        "## DPRの現時点の検索性能\n",
        "\n",
        "msmarco-passageでfine-tuningされたDPRを、nfcorpusでファインチューニングしていきます。\n",
        "\n",
        "その前に、今の時点ではどのくらいの性能が出ているのか見てみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQTkJFPylDJq"
      },
      "source": [
        "### Tevatronのスクリプトを用いて、nfcorpusで評価\n",
        "\n",
        "nfcorpusはBEIRというベンチマークに含まれているデータセットの一つです。\n",
        "\n",
        "Tevatronには検索モデルをBEIRで簡単に評価できるようなスクリプトがあるので使わせてもらいましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SvKbzz2q_8G",
        "outputId": "f5dfafea-93a9-4ccf-d7bd-07a8d95253ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: BASE_IR_MODEL=k-ush/tevatron_dpr\n"
          ]
        }
      ],
      "source": [
        "%env BASE_IR_MODEL=k-ush/tevatron_dpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNKrM6zok7ET",
        "outputId": "0068b11f-5dbb-4a97-ba29-e2b630b005ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-01 11:46:33.574172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:46:33.574229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:46:33.578707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:46:33.589802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:46:35.308756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading: 100% 736/736 [00:00<00:00, 4.39MB/s]\n",
            "Downloading: 100% 48.0/48.0 [00:00<00:00, 271kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 2.77MB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 10.5MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 15.0MB/s]\n",
            "03/01/2024 11:46:42 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:46:42 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "Downloading: 100% 418M/418M [00:06<00:00, 71.4MB/s]\n",
            "Downloading: 100% 3.69k/3.69k [00:00<00:00, 17.2MB/s]\n",
            "Downloading and preparing dataset beir_corpus/nfcorpus to /root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a...\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading:   0% 0.00/1.94M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading:   3% 60.4k/1.94M [00:00<00:03, 480kB/s]\u001b[A\n",
            "Downloading:  16% 314k/1.94M [00:00<00:01, 1.38MB/s]\u001b[A\n",
            "Downloading: 100% 1.94M/1.94M [00:00<00:00, 4.35MB/s]\n",
            "100% 1/1 [00:01<00:00,  1.73s/it]\n",
            "100% 1/1 [00:00<00:00, 25.01it/s]\n",
            "Dataset beir_corpus downloaded and prepared to /root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 108.15it/s]\n",
            "03/01/2024 11:46:53 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x79b322fffc70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  68% 26/38 [00:00<00:00, 251.78ex/s]\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 225.27ex/s]\n",
            "\n",
            "Running tokenization #1:  37% 14/38 [00:00<00:00, 124.84ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  42% 16/38 [00:00<00:00, 154.21ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 171.56ex/s]\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 195.92ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  29% 11/38 [00:00<00:00, 97.85ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  89% 34/38 [00:00<00:00, 284.32ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  58% 22/38 [00:00<00:00, 103.55ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 254.98ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  29% 11/38 [00:00<00:00, 104.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 115.27ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  26% 10/38 [00:00<00:00, 92.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  58% 22/38 [00:00<00:00, 104.84ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  21% 8/38 [00:00<00:00, 69.85ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  53% 20/38 [00:00<00:00, 91.49ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  32% 12/38 [00:00<00:00, 117.19ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 103.55ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  45% 17/38 [00:00<00:00, 80.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  58% 22/38 [00:00<00:00, 212.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 193.97ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 101.67ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 177.90ex/s]\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 106.52ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  45% 17/38 [00:00<00:00, 163.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 38/38 [00:00<00:00, 237.39ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 234.49ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.48it/s]\n",
            "2024-03-01 11:47:10.489612: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:47:10.489668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:47:10.491393: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:47:10.504629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:47:11.921624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:47:15 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:47:15 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:47:18 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 673.46it/s]\n",
            "03/01/2024 11:47:18 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7fcebc1a9000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  68% 26/38 [00:00<00:00, 251.54ex/s]\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 221.71ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  34% 13/38 [00:00<00:00, 128.10ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  74% 28/38 [00:00<00:00, 128.98ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  34% 13/38 [00:00<00:00, 125.35ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  82% 31/38 [00:00<00:00, 155.31ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 133.82ex/s]\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 154.18ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  29% 11/38 [00:00<00:00, 98.39ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  82% 31/38 [00:00<00:00, 157.00ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 148.26ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  34% 13/38 [00:00<00:00, 128.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  55% 21/38 [00:00<00:00, 95.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  82% 31/38 [00:00<00:00, 101.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 108.19ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  18% 7/38 [00:00<00:00, 67.65ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  87% 33/38 [00:00<00:00, 169.68ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 159.67ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 106.32ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  24% 9/38 [00:00<00:00, 85.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  53% 20/38 [00:00<00:00, 102.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  32% 12/38 [00:00<00:00, 108.94ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  55% 21/38 [00:00<00:00, 103.35ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 121.01ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 141.48ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  46% 17/37 [00:00<00:00, 166.92ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  79% 30/38 [00:00<00:00, 149.03ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 138.01ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 237.06ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 145.88ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.50it/s]\n",
            "2024-03-01 11:47:33.274010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:47:33.274059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:47:33.275307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:47:33.282597: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:47:34.379142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:47:39 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:47:39 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:47:41 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 641.33it/s]\n",
            "03/01/2024 11:47:41 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7ec0e016d000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  42% 16/38 [00:00<00:00, 150.96ex/s]\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1:  47% 18/38 [00:00<00:00, 172.36ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  66% 25/38 [00:00<00:00, 248.45ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 172.18ex/s]\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 167.83ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 97.03ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  26% 10/38 [00:00<00:00, 92.90ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  45% 17/38 [00:00<00:00, 78.03ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  53% 20/38 [00:00<00:00, 63.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  66% 25/38 [00:00<00:00, 61.42ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  13% 5/38 [00:00<00:00, 49.77ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  79% 30/38 [00:00<00:00, 76.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 88.88ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 71.97ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  26% 10/38 [00:00<00:00, 79.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  58% 22/38 [00:00<00:00, 199.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 200.65ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  55% 21/38 [00:00<00:00, 91.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  50% 19/38 [00:00<00:00, 179.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  92% 35/38 [00:00<00:00, 91.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 86.53ex/s]\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 115.36ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 172.60ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  32% 12/38 [00:00<00:00, 119.91ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  38% 14/37 [00:00<00:00, 137.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 231.98ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 149.32ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 217.04ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.50it/s]\n",
            "2024-03-01 11:47:56.606858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:47:56.606908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:47:56.608122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:47:56.615179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:47:57.622400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:48:02 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:48:02 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:48:04 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 666.71it/s]\n",
            "03/01/2024 11:48:04 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7e5e603a51b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #1:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 377.64ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  39% 15/38 [00:00<00:00, 132.68ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  50% 19/38 [00:00<00:00, 169.43ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 173.31ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 244.97ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  39% 15/38 [00:00<00:00, 149.79ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  95% 36/38 [00:00<00:00, 113.29ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 119.56ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  32% 12/38 [00:00<00:00, 113.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 175.06ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  47% 18/38 [00:00<00:00, 163.57ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  29% 11/38 [00:00<00:00, 102.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  71% 27/38 [00:00<00:00, 121.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  92% 35/38 [00:00<00:00, 135.63ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 143.78ex/s]\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 123.01ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  34% 13/38 [00:00<00:00, 125.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  74% 28/38 [00:00<00:00, 136.44ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  39% 15/38 [00:00<00:00, 145.85ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 125.31ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 163.81ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:  43% 16/37 [00:00<00:00, 156.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  92% 35/38 [00:00<00:00, 166.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 142.40ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 242.51ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 143.51ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.48it/s]\n",
            "2024-03-01 11:48:18.602534: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:48:18.602603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:48:18.603983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:48:18.610733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:48:19.613570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:48:23 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:48:23 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:48:26 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 676.06it/s]\n",
            "03/01/2024 11:48:26 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x783b50785000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 366.09ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1:  34% 13/38 [00:00<00:00, 113.46ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  34% 13/38 [00:00<00:00, 122.56ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  71% 27/38 [00:00<00:00, 119.91ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  37% 14/38 [00:00<00:00, 126.78ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  74% 28/38 [00:00<00:00, 137.54ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 125.22ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  29% 11/38 [00:00<00:00, 104.45ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  29% 11/38 [00:00<00:00, 94.42ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 128.61ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 131.81ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  58% 22/38 [00:00<00:00, 95.28ex/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 154.59ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  21% 8/38 [00:00<00:00, 66.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  39% 15/38 [00:00<00:00, 71.82ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  84% 32/38 [00:00<00:00, 91.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  50% 19/38 [00:00<00:00, 84.99ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 94.34ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  68% 26/38 [00:00<00:00, 84.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 92.97ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  58% 22/38 [00:00<00:00, 111.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  29% 11/38 [00:00<00:00, 98.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 98.52ex/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  51% 19/37 [00:00<00:00, 175.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  89% 34/38 [00:00<00:00, 111.96ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 111.62ex/s]\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 152.74ex/s]\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 154.39ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 156.47ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.47it/s]\n",
            "2024-03-01 11:48:40.475389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:48:40.475439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:48:40.476682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:48:40.483650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:48:41.477283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:48:45 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:48:45 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:48:47 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 522.13it/s]\n",
            "03/01/2024 11:48:47 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7f091c7fd000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0:  24% 9/38 [00:00<00:00, 84.81ex/s]\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1:  24% 9/38 [00:00<00:00, 77.00ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #0:  47% 18/38 [00:00<00:00, 85.99ex/s]\n",
            "\n",
            "Running tokenization #2:  21% 8/38 [00:00<00:00, 78.87ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1:  50% 19/38 [00:00<00:00, 88.92ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #0:  76% 29/38 [00:00<00:00, 92.11ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 97.08ex/s]\n",
            "\n",
            "Running tokenization #1:  76% 29/38 [00:00<00:00, 89.22ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  47% 18/38 [00:00<00:00, 80.82ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  18% 7/38 [00:00<00:00, 69.42ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  71% 27/38 [00:00<00:00, 87.27ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 83.25ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:  71% 27/38 [00:00<00:00, 82.44ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  26% 10/38 [00:00<00:00, 85.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  42% 16/38 [00:00<00:00, 78.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenization #2:  95% 36/38 [00:00<00:00, 74.11ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  95% 36/38 [00:00<00:00, 76.59ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 74.43ex/s]\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 76.95ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  50% 19/38 [00:00<00:00, 65.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  63% 24/38 [00:00<00:00, 68.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  32% 12/38 [00:00<00:00, 53.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  84% 32/38 [00:00<00:00, 72.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  74% 28/38 [00:00<00:00, 70.81ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 72.12ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  55% 21/38 [00:00<00:00, 59.64ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 71.35ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  37% 14/38 [00:00<00:00, 65.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  74% 28/38 [00:00<00:00, 62.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  18% 7/38 [00:00<00:00, 66.19ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  63% 24/38 [00:00<00:00, 76.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 66.11ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  42% 16/38 [00:00<00:00, 74.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  21% 8/38 [00:00<00:00, 73.51ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 78.48ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  68% 26/38 [00:00<00:00, 85.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  45% 17/38 [00:00<00:00, 80.92ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 92.34ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  82% 31/38 [00:00<00:00, 106.99ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:  32% 12/37 [00:00<00:00, 119.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 89.52ex/s] \n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 119.83ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 129.38ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.49it/s]\n",
            "2024-03-01 11:49:02.951627: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:49:02.951681: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:49:02.952888: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:49:02.959716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:49:03.963749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:49:07 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:49:07 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:49:10 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 672.27it/s]\n",
            "03/01/2024 11:49:10 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7bec403b9000> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 453.40ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "Running tokenization #1:  26% 10/38 [00:00<00:00, 99.97ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 508.02ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  76% 29/38 [00:00<00:00, 144.71ex/s]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  24% 9/38 [00:00<00:00, 88.30ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 149.15ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  45% 17/38 [00:00<00:00, 168.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  53% 20/38 [00:00<00:00, 97.25ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 194.65ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  58% 22/38 [00:00<00:00, 219.17ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  84% 32/38 [00:00<00:00, 100.86ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 102.62ex/s]\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 188.58ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  37% 14/38 [00:00<00:00, 128.96ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 183.14ex/s]\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 99.51ex/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  37% 14/38 [00:00<00:00, 133.50ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 212.61ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  32% 12/38 [00:00<00:00, 119.41ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 192.28ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 198.16ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 174.77ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.48it/s]\n",
            "2024-03-01 11:49:25.930046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:49:25.930099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:49:25.931853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:49:25.942479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:49:27.083793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:49:31 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:49:31 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "03/01/2024 11:49:33 - WARNING - datasets.builder -   Reusing dataset beir_corpus (/root/.cache/huggingface/datasets/Tevatron___beir_corpus/nfcorpus/1.1.0/02e1318cd9412cdf85d3f039bf36bec0af49ddeeab2279d4cf19fe556af6f29a)\n",
            "100% 1/1 [00:00<00:00, 555.10it/s]\n",
            "03/01/2024 11:49:33 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7d27f83111b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0:   0% 0/38 [00:00<?, ?ex/s]\n",
            "Running tokenization #0: 100% 38/38 [00:00<00:00, 571.16ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  37% 14/38 [00:00<00:00, 134.78ex/s]\u001b[A\n",
            "\n",
            "Running tokenization #2:  47% 18/38 [00:00<00:00, 175.38ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenization #3:  74% 28/38 [00:00<00:00, 251.35ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenization #1:  74% 28/38 [00:00<00:00, 132.48ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #2: 100% 38/38 [00:00<00:00, 192.44ex/s]\n",
            "Running tokenization #3: 100% 38/38 [00:00<00:00, 207.89ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #1: 100% 38/38 [00:00<00:00, 125.13ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5:  42% 16/38 [00:00<00:00, 159.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4:  63% 24/38 [00:00<00:00, 110.17ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #5: 100% 38/38 [00:00<00:00, 199.84ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  29% 11/38 [00:00<00:00, 109.68ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 38/38 [00:00<00:00, 118.53ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7:  42% 16/38 [00:00<00:00, 157.57ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6:  68% 26/38 [00:00<00:00, 132.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/38 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  32% 12/38 [00:00<00:00, 113.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 38/38 [00:00<00:00, 140.82ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 38/38 [00:00<00:00, 144.59ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  29% 11/38 [00:00<00:00, 97.81ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8:  71% 27/38 [00:00<00:00, 128.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11:   0% 0/37 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #10: 100% 37/37 [00:00<00:00, 325.61ex/s]\n",
            "Running tokenization #8: 100% 38/38 [00:00<00:00, 134.47ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:  63% 24/38 [00:00<00:00, 115.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 38/38 [00:00<00:00, 139.03ex/s]\n",
            "Running tokenization #11: 100% 37/37 [00:00<00:00, 210.81ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 8/8 [00:05<00:00,  1.48it/s]\n",
            "2024-03-01 11:49:48.010930: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:49:48.010988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:49:48.012203: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:49:48.019411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:49:49.186779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:49:53 - INFO - tevatron.modeling.encoder -   try loading tied weight\n",
            "03/01/2024 11:49:53 - INFO - tevatron.modeling.encoder -   loading model weight from k-ush/tevatron_dpr\n",
            "Downloading: 100% 3.61k/3.61k [00:00<00:00, 16.6MB/s]\n",
            "Downloading and preparing dataset beir/nfcorpus to /root/.cache/huggingface/datasets/Tevatron___beir/nfcorpus/1.0.0/bbd662c3d4cd99695bdc9c251b14cb18c0fcc7cb571d8373c5746b97789a4b9a...\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading: 100% 19.9k/19.9k [00:00<00:00, 352kB/s]\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "100% 1/1 [00:00<00:00, 1441.84it/s]\n",
            "Dataset beir downloaded and prepared to /root/.cache/huggingface/datasets/Tevatron___beir/nfcorpus/1.0.0/bbd662c3d4cd99695bdc9c251b14cb18c0fcc7cb571d8373c5746b97789a4b9a. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 884.87it/s]\n",
            "03/01/2024 11:49:57 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7da1c4273d00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenization #0: 100% 27/27 [00:00<00:00, 2638.48ex/s]\n",
            "\n",
            "Running tokenization #1: 100% 27/27 [00:00<00:00, 2603.36ex/s]\n",
            "\n",
            "\n",
            "Running tokenization #2:   0% 0/27 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #4: 100% 27/27 [00:00<00:00, 2270.24ex/s]\n",
            "Running tokenization #2: 100% 27/27 [00:00<00:00, 516.44ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3:   0% 0/27 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #3: 100% 27/27 [00:00<00:00, 513.49ex/s]\n",
            "Running tokenization #5: 100% 27/27 [00:00<00:00, 2228.60ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #6: 100% 27/27 [00:00<00:00, 874.47ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #7: 100% 27/27 [00:00<00:00, 2150.35ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #8: 100% 27/27 [00:00<00:00, 1038.64ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9:   0% 0/27 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #9: 100% 27/27 [00:00<00:00, 896.65ex/s]\n",
            "Running tokenization #10: 100% 27/27 [00:00<00:00, 2178.52ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenization #11: 100% 26/26 [00:00<00:00, 3353.69ex/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 6/6 [00:03<00:00,  1.62it/s]\n",
            "03/01/2024 11:50:09 - INFO - __main__ -   Pattern match found 8 files; loading them into index.\n",
            "Loading shards into index: 100% 8/8 [00:00<00:00, 376.53it/s]\n",
            "03/01/2024 11:50:09 - INFO - __main__ -   Index Search Start\n",
            "100% 6/6 [00:00<00:00, 55.45it/s]\n",
            "03/01/2024 11:50:09 - INFO - __main__ -   Index Search Finished\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "jtreceval-0.0.5-jar-with-dependencies.jar: 1.79MB [00:00, 5.11MB/s]                \n",
            "Downloading qrels from https://raw.githubusercontent.com/castorini/anserini-tools/master/topics-and-qrels/qrels.beir-v1.0.0-nfcorpus.test.txt\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-mrecall.100', '-mndcg_cut.10', '/root/.cache/anserini/topics-and-qrels/qrels.beir-v1.0.0-nfcorpus.test.txt', 'beir_embedding_k-ush/tevatron_dpr/rank.nfcorpus.trec']\n",
            "Results:\n",
            "recall_100            \tall\t0.1747\n",
            "ndcg_cut_10           \tall\t0.1620\n"
          ]
        }
      ],
      "source": [
        "!mkdir beir_embedding_k-ush\n",
        "!bash tevatron/scripts/eval_beir.sh $BASE_IR_MODEL nfcorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb5XKxDh6w6l"
      },
      "source": [
        "## DPRを訓練する\n",
        "\n",
        "それでは、DPRを訓練していきます。\n",
        "\n",
        "引き続きTevatronを使って、nfcorpusの訓練データセットでDPRを訓練していきますが、その前にnfcorpusの訓練データをTevatronで扱えるフォーマットに直す必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Utl_tyrCLIe"
      },
      "source": [
        "### 訓練データの整形\n",
        "Tevatronでの訓練データセットの形はDPRで使われる訓練データセットと似ており，以下の形式のjsonで構成されるjsonlで食わせます．内部的にはdatasetsのload_datasetを読んでいるので，load_datasetで読んで下記の構造のデータになればなんでも良いです．\n",
        "\n",
        "```\n",
        "{\n",
        "  query_id: ...,\n",
        "  query: ...,\n",
        "  positive_passages: [\n",
        "    {\n",
        "      docid: ...,\n",
        "      title: ...,\n",
        "      text: ...,\n",
        "    }\n",
        "  ],\n",
        "  negative_passages: [\n",
        "    {\n",
        "      docid: ...,\n",
        "      title: ...,\n",
        "      text: ...,\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "訓練データは，情報検索向けのデータセットを集めて，同じようなインターフェースからアクセスできるようにしているライブラリである[`ir-datasets`](https://ir-datasets.com/)を用いる．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A-pi0qXwDcV7",
        "outputId": "23017256-af5d-4d10-b4a3-a21ee5fa21f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.6-py3-none-any.whl (335 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.2/335.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.12.3)\n",
            "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
            "  Downloading inscriptis-2.4.0.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.66.2)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Collecting lz4>=3.1.10 (from ir_datasets)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting warc3-wet>=0.2.3 (from ir_datasets)\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir_datasets)\n",
            "  Downloading zlib-state-0.1.6.tar.gz (9.5 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ijson>=3.1.3 (from ir_datasets)\n",
            "  Downloading ijson-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyautocorpus>=0.1.1 (from ir_datasets)\n",
            "  Downloading pyautocorpus-0.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unlzw3>=0.2.1 (from ir_datasets)\n",
            "  Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (2024.2.2)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, zlib-state, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=8f129ac9271589c2b1384970a403b4a8df3c6f3fdf4f0cdbc49d553a9db5a6e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/d7/91/7ffb991df87e62355d945745035470ba2616aa3d83a250b5f9\n",
            "  Building wheel for zlib-state (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zlib-state: filename=zlib_state-0.1.6-cp310-cp310-linux_x86_64.whl size=21164 sha256=c0d929705f2b88955fcd20fedf16833dc6556381455907c1f2118b564cbad115\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/72/7e/aff80f26e926b6e1fb08dfb52aba03c0e058f5e2258deb50a9\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp310-cp310-linux_x86_64.whl size=53431 sha256=4a54c360cb53d114b81727acae11d8b7651ff98c1b1241d46f6030180d6c7151\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/df/c9/b39e40eccaf76dbd218556639a6dc81562226f4c6a64902c85\n",
            "Successfully built warc3-wet-clueweb09 zlib-state cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, pyautocorpus, lz4, inscriptis, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.2.3 inscriptis-2.4.0.1 ir_datasets-0.5.6 lz4-4.3.3 pyautocorpus-0.1.12 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install ir_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Htu3DtdDDRT1",
        "outputId": "a1fd9098-444c-4a52-e7d7-c59be5613ba9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO] [starting] building docstore\n",
            "[INFO] [starting] opening zip file\n",
            "[INFO] [starting] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\n",
            "docs_iter:   0%|                                      | 0/3633 [00:01<?, ?doc/s]\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 0.0%| 0.00/2.45M [00:00<?, ?B/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 2.0%| 49.2k/2.45M [00:00<00:07, 300kB/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 4.7%| 115k/2.45M [00:00<00:05, 417kB/s] \u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 10.7%| 262k/2.45M [00:00<00:03, 630kB/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 22.1%| 541k/2.45M [00:00<00:01, 961kB/s]\u001b[A\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: 45.5%| 1.11M/2.45M [00:00<00:00, 1.59MB/s]\u001b[A\n",
            "\n",
            "\u001b[A[INFO] [finished] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: [00:00] [2.45MB] [2.89MB/s]\n",
            "docs_iter:   0%|                                      | 0/3633 [00:02<?, ?doc/s]\n",
            "https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip: [00:00] [2.45MB] [2.86MB/s]\u001b[A\n",
            "[INFO] [finished] opening zip file [2.07s]\n",
            "docs_iter: 100%|█████████████████████████| 3633/3633 [00:02<00:00, 1601.14doc/s]\n",
            "[INFO] [finished] docs_iter: [00:02] [3633doc] [1600.40doc/s]\n",
            "[INFO] [finished] building docstore [2.28s]\n",
            "loading doc id: 100%|██████████| 3633/3633 [00:00<00:00, 173236.77it/s]\n",
            "[INFO] [starting] opening zip file\n",
            "[INFO] [finished] opening zip file [3ms]\n",
            "[INFO] [starting] opening zip file\n",
            "[INFO] [finished] opening zip file [8ms]\n",
            "loading query: 100%|██████████| 2590/2590 [00:00<00:00, 7389.73it/s]\n",
            "loading qrels: 100%|██████████| 110575/110575 [00:00<00:00, 350267.10it/s]\n",
            "writing train dataset: 100%|██████████| 1000/1000 [00:03<00:00, 324.12it/s]\n"
          ]
        }
      ],
      "source": [
        "from collections.abc import Callable\n",
        "from pathlib import Path\n",
        "from typing import Union, Dict, List, Set, Callable\n",
        "from collections import defaultdict, namedtuple\n",
        "import random\n",
        "import json\n",
        "\n",
        "import ir_datasets as irds\n",
        "from tqdm import tqdm\n",
        "\n",
        "Qrels = Dict[str, Dict[str, str]]\n",
        "\n",
        "class IrdsToTevatronDataset(object):\n",
        "  def __init__(self, dataset_key: str) -> None:\n",
        "    self.dataset = self._load_dataset(dataset_key)\n",
        "\n",
        "  def _load_dataset(self, dataset_key: str) -> irds.Dataset:\n",
        "    return irds.load(dataset_key)\n",
        "\n",
        "  def load_doc_ids(self) -> List[str]:\n",
        "    doc_ids = []\n",
        "    for doc in tqdm(self.dataset.docs_iter(), total=self.dataset.docs_count(), desc=\"loading doc id\"):\n",
        "      doc_ids.append(doc.doc_id)\n",
        "    return doc_ids\n",
        "\n",
        "  def load_query_table(self, query_field: str = \"text\") -> Dict[str, str]:\n",
        "    queries = {}\n",
        "    for query in tqdm(self.dataset.queries_iter(), total=self.dataset.queries_count(), desc=\"loading query\"):\n",
        "      queries[query.query_id] = getattr(query, query_field)\n",
        "    return queries\n",
        "\n",
        "  def load_qrels_table(self) -> Qrels:\n",
        "    qrels = defaultdict(dict)\n",
        "    for qrel in tqdm(self.dataset.qrels_iter(), total=self.dataset.qrels_count(), desc=\"loading qrels\"):\n",
        "      qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "    return qrels\n",
        "\n",
        "  def sample_random_negatives(self, doc_ids: List[str], exclude_doc_ids: Union[Set[str], List[str]], k: int = 1) -> List[str]:\n",
        "    exclude_doc_ids = set(exclude_doc_ids)\n",
        "    sample_source = [doc_id for doc_id in doc_ids if not doc_id in exclude_doc_ids]\n",
        "    negative_doc_ids = random.choices(sample_source, k=k)\n",
        "    return random.choices(sample_source, k=k)\n",
        "\n",
        "  def prepare_train_dataset(self, output_path: Union[str, Path], doc_preprocess: Callable[[namedtuple], str], queries_num: int = 500):\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    docstore = self.dataset.docs_store()\n",
        "    doc_ids = self.load_doc_ids()\n",
        "    queries = self.load_query_table()\n",
        "    qrels = self.load_qrels_table()\n",
        "\n",
        "    with output_path.open(\"w\") as fw:\n",
        "      total = min(len(queries), queries_num)\n",
        "      for i, qid in enumerate(tqdm(queries, desc=\"writing train dataset\", total=total)):\n",
        "        if i >= total: break\n",
        "\n",
        "        query = queries[qid]\n",
        "        relevant_doc_ids = [doc_id for doc_id in qrels[qid].keys() if qrels[qid][doc_id] > 0]\n",
        "        negative_ids = self.sample_random_negatives(doc_ids, relevant_doc_ids, k=len(relevant_doc_ids))\n",
        "        negatives = [doc_preprocess(doc) for docid, doc in docstore.get_many(negative_ids).items()]\n",
        "        positives = [doc_preprocess(doc) for docid, doc in docstore.get_many(relevant_doc_ids).items()]\n",
        "        train_json = {\n",
        "            \"query_id\": qid,\n",
        "            \"query\": query,\n",
        "            \"positive_passages\": positives,\n",
        "            \"negative_passages\": negatives\n",
        "        }\n",
        "        fw.write(json.dumps(train_json, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "dataset_key = \"beir/nfcorpus/train\"\n",
        "dataset_path = \"/content/nfcorpus.train.jsonl\"\n",
        "queries_num = 1000\n",
        "\n",
        "def nfcorpus_doc_preprocess(doc: namedtuple) -> str:\n",
        "  return {\n",
        "      \"docid\": doc.doc_id,\n",
        "      \"title\": doc.title,\n",
        "      \"text\": doc.text,\n",
        "  }\n",
        "\n",
        "irds_to_tev = IrdsToTevatronDataset(dataset_key)\n",
        "irds_to_tev.prepare_train_dataset(dataset_path, doc_preprocess=nfcorpus_doc_preprocess, queries_num=queries_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvLfJW4DDRkf"
      },
      "source": [
        "### **訓練の実行**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDTm2mkA7Kyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f6e3df-7f09-42d4-ae7b-c820a85fd671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-01 11:50:51.491266: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-01 11:50:51.491328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-01 11:50:51.493133: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-01 11:50:51.503318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-01 11:50:53.089354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/01/2024 11:50:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "03/01/2024 11:50:55 - INFO - __main__ -   Training/evaluation parameters TevatronTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_encode=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gc_p_chunk_size=32,\n",
            "gc_q_chunk_size=4,\n",
            "grad_cache=True,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=dpr_nfcorpus/runs/Mar01_11-50-55_2107ca36372e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "negatives_x_device=False,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=dpr_nfcorpus,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=dpr_nfcorpus,\n",
            "save_on_each_node=False,\n",
            "save_steps=10,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/01/2024 11:50:55 - INFO - __main__ -   MODEL parameters ModelArguments(model_name_or_path='k-ush/tevatron_dpr', config_name=None, tokenizer_name=None, cache_dir=None, untie_encoder=False, add_pooler=False, projection_in_dim=768, projection_out_dim=768, normalize=False, dtype='float32')\n",
            "Downloading: 100% 321/321 [00:00<00:00, 2.01MB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 3.26MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.56MB/s]\n",
            "Downloading: 100% 112/112 [00:00<00:00, 684kB/s]\n",
            "Downloading: 100% 4.43k/4.43k [00:00<00:00, 18.0MB/s]\n",
            "03/01/2024 11:51:00 - WARNING - datasets.builder -   Using custom data configuration default-026fa4a68544ff6f\n",
            "Downloading and preparing dataset wikipedia_nq/default to /root/.cache/huggingface/datasets/Tevatron___wikipedia_nq/default-026fa4a68544ff6f/0.0.1/f368df9f59f5cbb357d0542da8f1cad728db70e160c03a14799372a9ae66da71...\n",
            "Dataset wikipedia_nq downloaded and prepared to /root/.cache/huggingface/datasets/Tevatron___wikipedia_nq/default-026fa4a68544ff6f/0.0.1/f368df9f59f5cbb357d0542da8f1cad728db70e160c03a14799372a9ae66da71. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 126.25it/s]\n",
            "03/01/2024 11:51:02 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7d5766374160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on train dataset #0:   0% 0/84 [00:00<?, ?ex/s]\n",
            "Running tokenizer on train dataset #1:   0% 0/84 [00:00<?, ?ex/s]\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   0% 0/84 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:   0% 0/84 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:   1% 1/84 [00:00<00:15,  5.23ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   2% 2/83 [00:00<00:08,  9.44ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:   2% 2/84 [00:00<00:16,  4.92ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   1% 1/84 [00:00<00:43,  1.91ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   4% 3/83 [00:00<00:10,  7.64ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:   1% 1/84 [00:00<01:00,  1.37ex/s]\n",
            "Running tokenizer on train dataset #1:   1% 1/84 [00:00<00:53,  1.55ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:   6% 5/84 [00:00<00:08,  9.57ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   1% 1/83 [00:00<00:29,  2.82ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   1% 1/83 [00:00<00:25,  3.21ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:   0% 0/83 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   1% 1/83 [00:00<00:28,  2.91ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   5% 4/83 [00:00<00:21,  3.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:   8% 7/84 [00:00<00:10,  7.28ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   1% 1/83 [00:00<00:12,  6.39ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   2% 2/84 [00:01<00:47,  1.73ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   2% 2/83 [00:00<00:10,  7.66ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  11% 9/84 [00:01<00:08,  9.27ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   4% 3/83 [00:00<00:10,  7.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   5% 4/83 [00:00<00:09,  8.23ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   2% 2/83 [00:00<00:32,  2.47ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  13% 11/84 [00:01<00:08,  8.15ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   6% 5/83 [00:01<00:29,  2.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   4% 3/83 [00:01<00:34,  2.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:   1% 1/83 [00:01<01:59,  1.46s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  14% 12/84 [00:01<00:11,  6.42ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   7% 6/83 [00:01<00:22,  3.41ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:   2% 2/84 [00:01<01:19,  1.04ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   4% 3/83 [00:01<00:30,  2.66ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   4% 3/84 [00:01<00:52,  1.54ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:   4% 3/83 [00:01<00:34,  2.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  18% 15/84 [00:01<00:07,  9.44ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  20% 17/84 [00:02<00:06, 10.40ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   5% 4/83 [00:01<00:38,  2.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  23% 19/84 [00:02<00:08,  7.72ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   4% 3/83 [00:01<00:55,  1.43ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   5% 4/83 [00:02<00:37,  2.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   2% 2/83 [00:02<01:40,  1.24s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  25% 21/84 [00:02<00:08,  7.32ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:   7% 6/83 [00:02<00:29,  2.60ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:   8% 7/83 [00:02<00:45,  1.66ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   6% 5/83 [00:02<00:36,  2.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:   4% 3/84 [00:03<01:31,  1.13s/ex]\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  27% 23/84 [00:03<00:10,  5.94ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   5% 4/84 [00:03<01:15,  1.06ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  10% 8/83 [00:03<00:37,  1.98ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  30% 25/84 [00:03<00:09,  5.99ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  31% 26/84 [00:03<00:10,  5.47ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:   4% 3/84 [00:04<02:05,  1.54s/ex]\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  33% 28/84 [00:03<00:08,  6.71ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:   8% 7/83 [00:03<00:42,  1.78ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  35% 29/84 [00:04<00:08,  6.60ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   7% 6/83 [00:03<00:55,  1.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  37% 31/84 [00:04<00:07,  6.83ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   6% 5/84 [00:04<01:22,  1.04s/ex]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  38% 32/84 [00:04<00:07,  6.54ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:   4% 3/83 [00:03<01:30,  1.13s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:   8% 7/83 [00:04<00:48,  1.57ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   6% 5/83 [00:03<01:41,  1.30s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  40% 34/84 [00:04<00:06,  7.43ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  42% 35/84 [00:04<00:06,  7.84ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  11% 9/83 [00:04<01:05,  1.13ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   4% 3/83 [00:04<02:22,  1.78s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   6% 5/83 [00:04<01:39,  1.28s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  43% 36/84 [00:05<00:07,  6.77ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:   5% 4/84 [00:05<02:04,  1.56s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   5% 4/83 [00:04<02:00,  1.52s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  10% 8/83 [00:04<00:51,  1.45ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  13% 11/83 [00:05<00:42,  1.71ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:   6% 5/83 [00:04<01:07,  1.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   7% 6/84 [00:06<01:34,  1.21s/ex]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:   7% 6/83 [00:05<00:51,  1.49ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  11% 9/83 [00:05<00:47,  1.57ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  44% 37/84 [00:06<00:16,  2.90ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   6% 5/83 [00:05<01:38,  1.27s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  46% 39/84 [00:06<00:10,  4.19ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  49% 41/84 [00:06<00:08,  5.35ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   7% 6/83 [00:05<01:11,  1.08ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:   5% 4/84 [00:06<02:37,  1.97s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  13% 11/83 [00:06<00:33,  2.12ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  50% 42/84 [00:06<00:08,  4.90ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  14% 12/83 [00:06<00:27,  2.60ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:   8% 7/83 [00:06<00:57,  1.32ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  54% 45/84 [00:06<00:04,  7.88ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:   8% 7/84 [00:06<01:25,  1.11s/ex]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  57% 48/84 [00:06<00:03, 10.85ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   7% 6/83 [00:06<01:54,  1.49s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  10% 8/83 [00:06<00:45,  1.65ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  60% 50/84 [00:07<00:03, 10.56ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:   6% 5/84 [00:07<01:58,  1.50s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  11% 9/83 [00:06<00:41,  1.78ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  62% 52/84 [00:07<00:04,  7.27ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:   6% 5/84 [00:08<02:26,  1.85s/ex]\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  64% 54/84 [00:08<00:04,  6.52ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  12% 10/83 [00:07<00:41,  1.77ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  68% 57/84 [00:08<00:03,  8.46ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:   7% 6/84 [00:08<01:51,  1.43s/ex]\n",
            "\n",
            "Running tokenizer on train dataset #2:  11% 9/84 [00:08<01:11,  1.05ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  70% 59/84 [00:08<00:03,  7.24ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   7% 6/83 [00:07<02:48,  2.18s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  71% 60/84 [00:08<00:03,  7.41ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  16% 13/83 [00:07<00:27,  2.58ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  74% 62/84 [00:08<00:02,  8.28ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  11% 9/83 [00:08<01:35,  1.29s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  14% 12/83 [00:08<01:41,  1.44s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:   8% 7/83 [00:08<02:05,  1.65s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:   8% 7/84 [00:09<01:30,  1.17s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  12% 10/83 [00:08<01:15,  1.04s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  80% 67/84 [00:09<00:01, 12.03ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:   8% 7/83 [00:08<02:05,  1.65s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:   8% 7/84 [00:09<01:33,  1.22s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  10% 8/83 [00:08<01:30,  1.21s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  16% 13/83 [00:09<01:16,  1.09s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  12% 10/84 [00:09<01:07,  1.09ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  82% 69/84 [00:09<00:01, 10.20ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  13% 11/83 [00:09<01:08,  1.05ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  85% 71/84 [00:09<00:01,  7.80ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  86% 72/84 [00:09<00:01,  8.05ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  89% 75/84 [00:10<00:00, 11.01ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  10% 8/84 [00:10<01:24,  1.11s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  16% 13/83 [00:09<00:46,  1.51ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  17% 14/83 [00:09<00:42,  1.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  17% 14/83 [00:10<00:38,  1.80ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  13% 11/84 [00:10<01:10,  1.03ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  17% 14/83 [00:10<01:17,  1.13s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  92% 77/84 [00:10<00:00,  7.49ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  11% 9/83 [00:10<01:35,  1.29s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  10% 8/84 [00:10<01:42,  1.35s/ex]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  11% 9/84 [00:11<01:21,  1.09s/ex]\n",
            "\n",
            "Running tokenizer on train dataset #2:  14% 12/84 [00:11<01:07,  1.06ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  11% 9/84 [00:11<01:30,  1.20s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  18% 15/83 [00:11<00:47,  1.42ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  15% 13/84 [00:11<00:58,  1.22ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  12% 10/84 [00:12<01:11,  1.04ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  13% 11/83 [00:11<01:06,  1.08ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  19% 16/83 [00:12<01:11,  1.06s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  17% 14/84 [00:12<00:52,  1.34ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  12% 10/84 [00:12<01:25,  1.16s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  19% 16/83 [00:12<00:54,  1.22ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  23% 19/83 [00:12<00:36,  1.74ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  16% 13/83 [00:12<02:13,  1.91s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  24% 20/83 [00:12<00:30,  2.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  13% 11/84 [00:13<01:26,  1.18s/ex]\n",
            "Running tokenizer on train dataset #1:  13% 11/84 [00:13<01:25,  1.17s/ex]\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  19% 16/84 [00:13<00:48,  1.41ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  11% 9/83 [00:13<02:36,  2.11s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  17% 14/83 [00:13<01:59,  1.73s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  18% 15/83 [00:13<01:27,  1.29s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  14% 12/84 [00:14<01:13,  1.02s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  11% 9/83 [00:13<02:37,  2.12s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  20% 17/84 [00:14<00:46,  1.44ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  19% 16/83 [00:14<01:07,  1.01s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  96% 81/84 [00:14<00:01,  1.51ex/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  14% 12/84 [00:15<01:25,  1.19s/ex]\n",
            "\n",
            "Running tokenizer on train dataset #2:  21% 18/84 [00:15<00:42,  1.54ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  12% 10/83 [00:14<02:14,  1.84s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3:  98% 82/84 [00:15<00:01,  1.59ex/s]\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  15% 13/84 [00:15<01:11,  1.00s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  13% 11/83 [00:14<01:44,  1.45s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #3: 100% 84/84 [00:15<00:00,  5.37ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  22% 18/83 [00:15<00:54,  1.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  23% 19/84 [00:16<00:51,  1.27ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  13% 11/83 [00:15<01:55,  1.61s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  17% 14/84 [00:16<01:09,  1.01ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  14% 12/83 [00:16<01:38,  1.39s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  15% 13/84 [00:16<01:38,  1.38s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  18% 15/83 [00:16<00:46,  1.45ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  17% 14/84 [00:17<01:24,  1.21s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  20% 17/83 [00:17<02:04,  1.89s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  25% 21/84 [00:17<00:49,  1.28ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  22% 18/83 [00:17<01:31,  1.41s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  14% 12/83 [00:17<02:47,  2.36s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  19% 16/83 [00:17<00:55,  1.21ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  26% 22/84 [00:18<00:52,  1.17ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  18% 15/84 [00:18<01:24,  1.23s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  24% 20/83 [00:18<01:02,  1.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  22% 18/83 [00:18<00:43,  1.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  24% 20/83 [00:18<00:29,  2.10ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  19% 16/84 [00:19<01:29,  1.32s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  19% 16/84 [00:20<01:20,  1.18s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  25% 21/83 [00:19<01:00,  1.02ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  27% 22/83 [00:19<00:27,  2.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  27% 23/84 [00:20<01:06,  1.09s/ex]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  28% 23/83 [00:19<00:24,  2.47ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  30% 25/84 [00:20<00:37,  1.58ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  29% 24/83 [00:20<00:21,  2.74ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  20% 17/84 [00:20<01:29,  1.34s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  30% 25/83 [00:20<00:24,  2.41ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  31% 26/83 [00:20<00:19,  2.99ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  34% 28/83 [00:20<00:12,  4.47ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  20% 17/84 [00:21<01:33,  1.39s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  35% 29/83 [00:21<00:13,  3.90ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  32% 27/84 [00:22<00:40,  1.42ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  21% 18/84 [00:22<01:28,  1.35s/ex]\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  33% 28/84 [00:22<00:30,  1.84ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  23% 19/83 [00:21<02:43,  2.56s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  21% 18/84 [00:22<01:19,  1.21s/ex]\n",
            "\n",
            "Running tokenizer on train dataset #2:  36% 30/84 [00:22<00:22,  2.38ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  23% 19/84 [00:22<01:13,  1.13s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  27% 22/83 [00:22<01:38,  1.62s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  37% 31/84 [00:23<00:20,  2.53ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  28% 23/83 [00:23<01:12,  1.21s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  25% 21/83 [00:23<02:54,  2.81s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  23% 19/84 [00:23<01:15,  1.17s/ex]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  27% 22/83 [00:23<02:11,  2.15s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  38% 32/84 [00:23<00:23,  2.18ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  29% 24/83 [00:23<00:59,  1.01s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  36% 30/83 [00:23<00:35,  1.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  24% 20/83 [00:23<02:19,  2.21s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  24% 20/84 [00:24<01:13,  1.15s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  28% 23/83 [00:24<01:42,  1.70s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  37% 31/83 [00:23<00:32,  1.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  40% 33/83 [00:23<00:19,  2.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  40% 34/84 [00:24<00:21,  2.36ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  25% 21/83 [00:24<01:49,  1.77s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  30% 25/83 [00:24<00:54,  1.05ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  41% 34/83 [00:23<00:16,  2.89ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  25% 21/84 [00:24<01:03,  1.01s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  24% 20/84 [00:24<01:15,  1.19s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  19% 16/83 [00:24<01:50,  1.65s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  43% 36/83 [00:24<00:12,  3.88ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  42% 35/84 [00:25<00:22,  2.15ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  29% 24/83 [00:25<01:29,  1.51s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  22% 18/83 [00:24<02:04,  1.91s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  20% 17/83 [00:24<01:31,  1.39s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  31% 26/83 [00:25<00:49,  1.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  43% 36/84 [00:25<00:20,  2.33ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  26% 22/84 [00:25<00:57,  1.07ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  25% 21/84 [00:25<01:08,  1.09s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  24% 20/83 [00:24<01:17,  1.22s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  45% 37/83 [00:25<00:18,  2.45ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  27% 22/83 [00:25<00:51,  1.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  26% 22/84 [00:26<01:02,  1.02s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  27% 23/84 [00:26<00:47,  1.29ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  46% 38/83 [00:26<00:24,  1.86ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  45% 38/84 [00:26<00:23,  1.94ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  28% 23/83 [00:25<00:46,  1.30ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  27% 23/84 [00:26<01:05,  1.08s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  47% 39/83 [00:26<00:22,  1.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  46% 39/84 [00:27<00:22,  1.99ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  48% 40/83 [00:26<00:17,  2.41ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  52% 43/83 [00:26<00:09,  4.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  48% 40/84 [00:27<00:21,  2.05ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  49% 41/84 [00:28<00:18,  2.32ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  50% 42/84 [00:28<00:14,  2.89ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  29% 24/84 [00:28<01:00,  1.01s/ex]\n",
            "Running tokenizer on train dataset #1:  29% 24/84 [00:28<01:12,  1.20s/ex]\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  51% 43/84 [00:28<00:15,  2.68ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  53% 44/83 [00:27<00:15,  2.54ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  30% 25/84 [00:28<00:50,  1.18ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  58% 48/83 [00:28<00:06,  5.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  60% 50/83 [00:28<00:05,  5.81ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  52% 44/84 [00:29<00:16,  2.40ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  33% 27/83 [00:28<01:32,  1.66s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  30% 25/84 [00:29<01:04,  1.10s/ex]\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  54% 45/84 [00:29<00:14,  2.76ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  55% 46/84 [00:29<00:13,  2.81ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  33% 27/83 [00:29<01:37,  1.74s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  31% 26/84 [00:30<00:55,  1.05ex/s]\n",
            "\n",
            "Running tokenizer on train dataset #2:  56% 47/84 [00:30<00:14,  2.56ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  31% 26/84 [00:30<01:01,  1.06s/ex]\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  57% 48/84 [00:30<00:11,  3.11ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  61% 51/84 [00:30<00:07,  4.59ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  62% 52/84 [00:30<00:06,  5.11ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  27% 22/83 [00:30<03:10,  3.12s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  65% 55/84 [00:30<00:03,  8.36ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  32% 27/84 [00:31<00:58,  1.03s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  28% 23/83 [00:30<02:18,  2.32s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  68% 57/84 [00:31<00:03,  6.88ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  35% 29/83 [00:31<01:13,  1.36s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  29% 24/83 [00:30<01:39,  1.69s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  36% 30/83 [00:31<00:57,  1.09s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  70% 59/84 [00:31<00:03,  7.35ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  32% 27/84 [00:31<01:09,  1.21s/ex]\n",
            "\n",
            "Running tokenizer on train dataset #0:  33% 28/84 [00:32<00:49,  1.12ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  39% 32/83 [00:31<00:38,  1.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  35% 29/83 [00:31<01:25,  1.57s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  76% 64/84 [00:32<00:02,  7.28ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  36% 30/83 [00:32<01:04,  1.21s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  40% 33/83 [00:32<00:32,  1.54ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  41% 34/83 [00:32<00:27,  1.81ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  77% 65/84 [00:32<00:03,  5.79ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  30% 25/83 [00:32<01:27,  1.50s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  79% 66/84 [00:32<00:03,  5.98ex/s]\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  35% 29/84 [00:33<00:51,  1.07ex/s]\n",
            "\n",
            "Running tokenizer on train dataset #2:  81% 68/84 [00:33<00:02,  5.48ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  39% 32/83 [00:33<00:47,  1.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  83% 70/84 [00:33<00:02,  6.40ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  36% 30/84 [00:34<00:51,  1.06ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  14% 12/83 [00:33<06:12,  5.25s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  87% 73/84 [00:34<00:01,  5.87ex/s]\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  88% 74/84 [00:34<00:01,  6.41ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  40% 33/83 [00:33<00:45,  1.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  37% 31/84 [00:34<00:48,  1.09ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  43% 36/83 [00:34<00:37,  1.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  34% 28/83 [00:34<00:54,  1.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  45% 37/83 [00:34<00:30,  1.49ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  35% 29/84 [00:35<01:25,  1.55s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  46% 38/83 [00:35<00:24,  1.85ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   5% 4/83 [00:34<17:07, 13.00s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   6% 5/83 [00:35<10:56,  8.41s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  24% 20/83 [00:35<02:39,  2.53s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  38% 32/84 [00:36<00:51,  1.01ex/s]\n",
            "\n",
            "Running tokenizer on train dataset #2:  89% 75/84 [00:36<00:05,  1.79ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  25% 21/83 [00:35<02:09,  2.08s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  36% 30/84 [00:36<01:20,  1.49s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  27% 22/83 [00:36<01:50,  1.82s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  49% 41/83 [00:36<00:26,  1.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  16% 13/83 [00:36<05:33,  4.77s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  30% 25/83 [00:36<00:53,  1.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  39% 33/84 [00:37<00:59,  1.16s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  35% 29/83 [00:37<01:21,  1.51s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  36% 30/83 [00:37<01:00,  1.15s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  90% 76/84 [00:38<00:07,  1.02ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  31% 26/83 [00:38<00:54,  1.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  40% 34/84 [00:38<00:56,  1.13s/ex]\n",
            "Running tokenizer on train dataset #1:  38% 32/84 [00:38<01:05,  1.26s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  37% 31/83 [00:38<00:48,  1.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  30% 25/83 [00:37<03:12,  3.32s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  93% 78/84 [00:39<00:04,  1.43ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  33% 27/83 [00:38<01:49,  1.95s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  34% 28/83 [00:38<01:24,  1.54s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  36% 30/83 [00:38<00:50,  1.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  94% 79/84 [00:39<00:03,  1.61ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  40% 33/83 [00:39<00:38,  1.30ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  42% 35/83 [00:39<00:23,  2.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  42% 35/83 [00:39<01:20,  1.68s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  39% 33/84 [00:39<01:04,  1.27s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  43% 36/83 [00:39<01:03,  1.35s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  42% 35/84 [00:40<01:02,  1.28s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  45% 37/83 [00:39<00:17,  2.64ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  45% 37/83 [00:40<00:50,  1.09s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  46% 38/83 [00:40<00:38,  1.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  43% 36/84 [00:40<00:51,  1.08s/ex]\n",
            "Running tokenizer on train dataset #1:  40% 34/84 [00:41<01:00,  1.21s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  51% 42/83 [00:41<01:00,  1.47s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  33% 27/83 [00:40<01:16,  1.37s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  48% 40/83 [00:40<00:28,  1.53ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  52% 43/83 [00:41<00:47,  1.18s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  49% 41/83 [00:41<00:24,  1.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  17% 14/83 [00:40<05:24,  4.71s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #0:  44% 37/84 [00:42<00:54,  1.15s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  34% 28/83 [00:41<01:10,  1.28s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  42% 35/84 [00:42<01:00,  1.23s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  55% 46/83 [00:42<00:25,  1.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  59% 49/83 [00:42<00:15,  2.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  43% 36/84 [00:42<00:50,  1.05s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  45% 38/84 [00:43<00:47,  1.04s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  37% 31/83 [00:42<01:21,  1.56s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  48% 40/83 [00:42<00:25,  1.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  98% 82/84 [00:43<00:02,  1.03s/ex]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  39% 32/83 [00:42<01:01,  1.21s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  35% 29/83 [00:43<01:07,  1.25s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  44% 37/84 [00:43<00:42,  1.11ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   7% 6/83 [00:43<10:35,  8.26s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  41% 34/83 [00:42<00:38,  1.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  61% 51/83 [00:43<00:14,  2.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  37% 31/83 [00:43<00:40,  1.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  19% 16/83 [00:42<03:29,  3.13s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  46% 39/84 [00:43<00:44,  1.00ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  42% 35/83 [00:42<00:32,  1.49ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2:  99% 83/84 [00:44<00:00,  1.04ex/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  39% 32/83 [00:43<00:35,  1.44ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:   8% 7/83 [00:43<07:16,  5.74s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  51% 42/83 [00:43<00:22,  1.80ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  10% 8/83 [00:43<04:58,  3.98s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Running tokenizer on train dataset #2: 100% 84/84 [00:44<00:00,  1.89ex/s]\n",
            "\n",
            "Running tokenizer on train dataset #1:  45% 38/84 [00:44<00:42,  1.08ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  51% 42/83 [00:44<00:47,  1.15s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  12% 10/83 [00:44<02:36,  2.15s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  53% 44/83 [00:44<00:27,  1.44ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  52% 43/83 [00:44<00:20,  1.93ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  48% 40/84 [00:44<00:42,  1.03ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  13% 11/83 [00:44<02:01,  1.68s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  55% 46/83 [00:44<00:10,  3.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  63% 52/83 [00:44<00:18,  1.66ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  14% 12/83 [00:44<01:30,  1.27s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  63% 52/83 [00:44<01:22,  2.67s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  57% 47/83 [00:44<00:15,  2.30ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  64% 53/83 [00:44<00:15,  1.96ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  46% 39/84 [00:45<00:39,  1.14ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  65% 54/83 [00:45<00:12,  2.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  65% 54/83 [00:44<00:53,  1.86s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  17% 14/83 [00:44<00:54,  1.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  41% 34/83 [00:45<00:33,  1.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  66% 55/83 [00:45<00:10,  2.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  19% 16/83 [00:45<00:35,  1.87ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  49% 41/84 [00:45<00:42,  1.02ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  20% 17/83 [00:45<00:31,  2.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  43% 36/83 [00:44<00:46,  1.02ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  57% 47/83 [00:45<00:15,  2.36ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  69% 57/83 [00:45<00:29,  1.14s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  22% 18/83 [00:45<00:26,  2.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  48% 40/84 [00:46<00:38,  1.15ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  67% 56/83 [00:45<00:11,  2.32ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  60% 50/83 [00:45<00:13,  2.39ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  69% 57/83 [00:46<00:10,  2.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  50% 42/84 [00:46<00:39,  1.05ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  61% 51/83 [00:46<00:13,  2.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  23% 19/83 [00:45<01:59,  1.87s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  63% 52/83 [00:46<00:11,  2.79ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  49% 41/84 [00:46<00:36,  1.16ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  64% 53/83 [00:46<00:09,  3.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  51% 43/84 [00:47<00:36,  1.13ex/s]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  71% 59/83 [00:47<00:10,  2.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  70% 58/83 [00:46<00:31,  1.25s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  72% 60/83 [00:47<00:11,  2.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  50% 42/84 [00:48<00:40,  1.03ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  71% 59/83 [00:47<00:27,  1.16s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  24% 20/83 [00:47<01:55,  1.84s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  52% 44/84 [00:48<00:39,  1.02ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  45% 37/83 [00:47<01:06,  1.45s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  73% 61/83 [00:48<00:12,  1.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  46% 38/83 [00:48<00:52,  1.17s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  75% 62/83 [00:48<00:13,  1.52ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  48% 40/83 [00:48<00:30,  1.40ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  59% 49/83 [00:48<00:30,  1.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  76% 63/83 [00:48<00:11,  1.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  51% 43/84 [00:49<00:45,  1.10s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  60% 50/83 [00:48<00:25,  1.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  75% 62/83 [00:49<00:13,  1.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  77% 64/83 [00:48<00:09,  2.08ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  49% 41/83 [00:48<00:28,  1.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  61% 51/83 [00:49<00:21,  1.51ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  78% 65/83 [00:49<00:08,  2.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  63% 52/83 [00:49<00:16,  1.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  81% 67/83 [00:49<00:04,  3.53ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  27% 22/83 [00:49<01:28,  1.46s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  64% 53/83 [00:49<00:14,  2.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  54% 45/84 [00:50<00:52,  1.36s/ex]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  76% 63/83 [00:50<00:17,  1.16ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  54% 45/84 [00:51<00:42,  1.09s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  82% 68/83 [00:51<00:09,  1.57ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  77% 64/83 [00:51<00:16,  1.13ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  55% 46/84 [00:52<00:52,  1.39s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  28% 23/83 [00:51<01:39,  1.66s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  80% 66/83 [00:52<00:11,  1.50ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  29% 24/83 [00:51<01:17,  1.32s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  55% 46/84 [00:53<00:46,  1.23s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  65% 54/83 [00:52<00:33,  1.16s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  56% 47/84 [00:53<00:49,  1.34s/ex]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  82% 68/83 [00:53<00:08,  1.85ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  66% 55/83 [00:53<00:27,  1.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  83% 69/83 [00:53<00:06,  2.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  57% 48/84 [00:54<00:41,  1.15s/ex]\n",
            "Running tokenizer on train dataset #1:  56% 47/84 [00:54<00:42,  1.15s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  43% 36/83 [00:54<01:38,  2.09s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  86% 71/83 [00:53<00:08,  1.35ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  51% 42/83 [00:53<01:11,  1.73s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  57% 48/84 [00:54<00:34,  1.04ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  54% 45/83 [00:53<00:32,  1.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  58% 49/84 [00:55<00:37,  1.06s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  87% 72/83 [00:54<00:07,  1.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  69% 57/83 [00:54<00:20,  1.26ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  58% 49/84 [00:55<00:30,  1.16ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  60% 50/84 [00:55<00:31,  1.09ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  57% 47/83 [00:55<00:28,  1.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  61% 51/84 [00:56<00:27,  1.21ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  59% 49/83 [00:55<00:19,  1.77ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  60% 50/84 [00:56<00:31,  1.09ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  89% 74/83 [00:55<00:06,  1.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  65% 54/83 [00:56<01:16,  2.63s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  62% 52/84 [00:57<00:25,  1.23ex/s]\n",
            "Running tokenizer on train dataset #1:  61% 51/84 [00:57<00:29,  1.12ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  67% 56/83 [00:57<00:42,  1.57s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  69% 57/83 [00:57<00:30,  1.19s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  63% 53/84 [00:57<00:23,  1.31ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  90% 75/83 [00:57<00:06,  1.23ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  62% 52/84 [00:57<00:25,  1.26ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  70% 58/83 [00:57<00:23,  1.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  92% 76/83 [00:57<00:04,  1.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  63% 53/84 [00:58<00:23,  1.34ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  71% 59/83 [00:58<00:20,  1.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  64% 54/84 [00:58<00:23,  1.26ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  73% 61/83 [00:58<00:10,  2.05ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  75% 62/83 [00:58<00:08,  2.35ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  94% 78/83 [00:58<00:02,  1.84ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  64% 54/84 [00:59<00:20,  1.49ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  65% 55/84 [00:59<00:22,  1.27ex/s]\n",
            "Running tokenizer on train dataset #0:  68% 57/84 [01:00<00:20,  1.30ex/s]\n",
            "Running tokenizer on train dataset #1:  67% 56/84 [01:01<00:24,  1.15ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  30% 25/83 [01:00<03:01,  3.12s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  31% 26/83 [01:00<02:12,  2.32s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  69% 58/84 [01:01<00:20,  1.26ex/s]\n",
            "Running tokenizer on train dataset #1:  68% 57/84 [01:01<00:22,  1.19ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  34% 28/83 [01:00<01:12,  1.31s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  95% 79/83 [01:01<00:04,  1.25s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  96% 80/83 [01:01<00:02,  1.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  45% 37/83 [01:01<02:30,  3.27s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  71% 60/84 [01:04<00:23,  1.00ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  46% 38/83 [01:03<02:10,  2.90s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  47% 39/83 [01:04<01:40,  2.29s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  73% 61/84 [01:05<00:24,  1.08s/ex]\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  84% 70/83 [01:05<00:50,  3.86s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  75% 63/84 [01:07<00:21,  1.02s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  48% 40/83 [01:06<01:43,  2.40s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  73% 61/84 [01:07<00:29,  1.26s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  81% 67/83 [01:06<00:24,  1.52s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  23% 19/83 [01:06<06:03,  5.68s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  82% 68/83 [01:07<00:19,  1.28s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  76% 64/84 [01:07<00:17,  1.14ex/s]\n",
            "Running tokenizer on train dataset #1:  74% 62/84 [01:07<00:23,  1.07s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9:  98% 81/83 [01:07<00:05,  2.53s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  75% 63/84 [01:08<00:19,  1.06ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  77% 65/84 [01:09<00:18,  1.01ex/s]\n",
            "Running tokenizer on train dataset #1:  76% 64/84 [01:09<00:17,  1.12ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  79% 66/84 [01:09<00:17,  1.04ex/s]\n",
            "Running tokenizer on train dataset #1:  77% 65/84 [01:10<00:16,  1.16ex/s]\u001b[A\n",
            "Running tokenizer on train dataset #1:  79% 66/84 [01:10<00:13,  1.32ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  81% 68/84 [01:11<00:12,  1.29ex/s]\n",
            "Running tokenizer on train dataset #1:  80% 67/84 [01:11<00:11,  1.44ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  73% 61/83 [01:10<01:14,  3.38s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  53% 44/83 [01:10<00:54,  1.41s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  77% 64/83 [01:10<00:35,  1.86s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  81% 68/84 [01:11<00:10,  1.55ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  80% 66/83 [01:11<00:22,  1.35s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  81% 67/83 [01:11<00:18,  1.13s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  83% 70/84 [01:12<00:09,  1.40ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  82% 68/83 [01:11<00:15,  1.03s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  82% 69/84 [01:12<00:10,  1.43ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  85% 71/84 [01:13<00:09,  1.35ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  84% 70/83 [01:12<00:23,  1.83s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  87% 72/83 [01:13<00:13,  1.24s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  84% 70/83 [01:12<00:10,  1.19ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  89% 74/83 [01:13<00:07,  1.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  87% 72/83 [01:13<00:05,  1.89ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  86% 72/84 [01:14<00:09,  1.30ex/s]\n",
            "Running tokenizer on train dataset #0:  87% 73/84 [01:15<00:08,  1.27ex/s]\n",
            "Running tokenizer on train dataset #1:  86% 72/84 [01:15<00:09,  1.33ex/s]\u001b[A\n",
            "Running tokenizer on train dataset #1:  87% 73/84 [01:15<00:08,  1.36ex/s]\u001b[A\n",
            "Running tokenizer on train dataset #0:  88% 74/84 [01:16<00:09,  1.04ex/s]\n",
            "Running tokenizer on train dataset #1:  89% 75/84 [01:16<00:05,  1.54ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  61% 51/83 [01:16<02:30,  4.72s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  89% 75/84 [01:17<00:09,  1.05s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  63% 52/83 [01:16<01:53,  3.66s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  88% 73/83 [01:16<00:13,  1.31s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  90% 75/83 [01:17<00:11,  1.47s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  92% 76/83 [01:17<00:08,  1.23s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  93% 77/83 [01:17<00:05,  1.01ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  90% 76/84 [01:18<00:07,  1.02ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  94% 78/83 [01:18<00:04,  1.13ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  64% 53/83 [01:17<01:31,  3.07s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #9: 100% 83/83 [01:18<00:00,  1.05ex/s]\n",
            "Running tokenizer on train dataset #0:  90% 76/84 [01:19<00:10,  1.35s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  39% 32/83 [01:18<02:44,  3.23s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  90% 75/83 [01:19<00:09,  1.24s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  40% 33/83 [01:19<02:06,  2.54s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  92% 77/84 [01:20<00:07,  1.13s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  41% 34/83 [01:19<01:39,  2.03s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  93% 78/84 [01:21<00:06,  1.09s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  42% 35/83 [01:20<01:19,  1.66s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  92% 77/84 [01:21<00:10,  1.56s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5:  96% 80/83 [01:21<00:03,  1.07s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  43% 36/83 [01:21<01:10,  1.49s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  93% 78/84 [01:22<00:08,  1.36s/ex]\n",
            "Running tokenizer on train dataset #1:  94% 79/84 [01:22<00:06,  1.21s/ex]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  24% 20/83 [01:22<08:39,  8.25s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  25% 21/83 [01:22<06:15,  6.06s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  86% 71/83 [01:23<01:35,  7.95s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  27% 22/83 [01:22<04:29,  4.41s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  45% 37/83 [01:22<01:05,  1.42s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  28% 23/83 [01:22<03:11,  3.19s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  94% 79/84 [01:23<00:06,  1.34s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  55% 46/83 [01:23<02:41,  4.36s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #1:  96% 81/84 [01:24<00:02,  1.09ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  92% 76/83 [01:23<00:13,  1.96s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Running tokenizer on train dataset #0:  95% 80/84 [01:24<00:04,  1.15s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #5: 100% 83/83 [01:24<00:00,  1.02s/ex]\n",
            "\n",
            "Running tokenizer on train dataset #1:  99% 83/84 [01:24<00:00,  1.47ex/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  29% 24/83 [01:24<02:38,  2.68s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  96% 81/84 [01:25<00:03,  1.04s/ex]\n",
            "Running tokenizer on train dataset #1: 100% 84/84 [01:25<00:00,  1.02s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  46% 38/83 [01:24<01:15,  1.67s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  30% 25/83 [01:25<02:04,  2.14s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  47% 39/83 [01:24<00:53,  1.22s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  31% 26/83 [01:25<01:28,  1.56s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  58% 48/83 [01:25<01:32,  2.65s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  98% 82/84 [01:26<00:01,  1.05ex/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  35% 29/83 [01:25<00:34,  1.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  36% 30/83 [01:25<00:27,  1.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  39% 32/83 [01:26<00:20,  2.52ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  40% 33/83 [01:26<00:16,  2.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  48% 40/83 [01:26<00:52,  1.22s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0:  99% 83/84 [01:27<00:01,  1.00s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  52% 43/83 [01:26<00:23,  1.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #0: 100% 84/84 [01:28<00:00,  1.05s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  41% 34/83 [01:27<00:27,  1.75ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  42% 35/83 [01:28<00:24,  1.95ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  45% 37/83 [01:28<00:17,  2.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  46% 38/83 [01:28<00:17,  2.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  55% 46/83 [01:28<00:26,  1.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  87% 72/83 [01:29<01:21,  7.45s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  48% 40/83 [01:29<00:12,  3.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  59% 49/83 [01:29<01:40,  2.95s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  57% 47/83 [01:28<00:20,  1.74ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  49% 41/83 [01:29<00:10,  4.00ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  90% 75/83 [01:29<00:26,  3.33s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  51% 42/83 [01:30<00:15,  2.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  93% 77/83 [01:30<00:13,  2.31s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  52% 43/83 [01:30<00:16,  2.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  94% 78/83 [01:30<00:09,  1.92s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  53% 44/83 [01:30<00:12,  3.03ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  57% 47/83 [01:30<00:07,  5.08ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  95% 79/83 [01:31<00:09,  2.27s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  58% 48/83 [01:31<00:38,  1.10s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8:  98% 81/83 [01:31<00:03,  1.58s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  59% 49/83 [01:31<00:31,  1.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  65% 54/83 [01:32<02:53,  5.99s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  60% 50/83 [01:32<00:25,  1.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  66% 55/83 [01:32<02:03,  4.40s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #8: 100% 83/83 [01:33<00:00,  1.13s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  60% 50/83 [01:34<01:59,  3.61s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  95% 79/83 [01:34<00:09,  2.31s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  59% 49/83 [01:34<00:24,  1.36ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  60% 50/83 [01:34<00:20,  1.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  96% 80/83 [01:35<00:05,  1.86s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  65% 54/83 [01:34<00:09,  3.17ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  69% 57/83 [01:35<01:21,  3.13s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  70% 58/83 [01:35<01:00,  2.41s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  67% 56/83 [01:35<00:10,  2.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  61% 51/83 [01:35<01:35,  2.99s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  70% 58/83 [01:36<00:07,  3.32ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  72% 60/83 [01:35<00:34,  1.49s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  64% 53/83 [01:36<00:50,  1.68s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  72% 60/83 [01:36<00:05,  3.93ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  73% 61/83 [01:36<00:28,  1.28s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  61% 51/83 [01:36<00:56,  1.76s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  75% 62/83 [01:36<00:21,  1.01s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  76% 63/83 [01:36<00:15,  1.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  64% 53/83 [01:36<00:30,  1.01s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  66% 55/83 [01:36<00:17,  1.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  69% 57/83 [01:36<00:11,  2.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  77% 64/83 [01:36<00:13,  1.39ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  70% 58/83 [01:37<00:10,  2.33ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  71% 59/83 [01:37<00:11,  2.12ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  67% 56/83 [01:38<00:32,  1.22s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  70% 58/83 [01:38<00:21,  1.15ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  78% 65/83 [01:38<00:15,  1.19ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  73% 61/83 [01:38<00:07,  2.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  82% 68/83 [01:38<00:06,  2.36ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  83% 69/83 [01:38<00:05,  2.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  73% 61/83 [01:39<00:12,  1.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  76% 63/83 [01:39<00:08,  2.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  84% 70/83 [01:39<00:06,  2.08ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  86% 71/83 [01:39<00:04,  2.50ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  80% 66/83 [01:40<00:06,  2.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  88% 73/83 [01:40<00:03,  2.65ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  89% 74/83 [01:40<00:03,  2.63ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4:  98% 81/83 [01:41<00:06,  3.03s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #4: 100% 83/83 [01:41<00:00,  1.22s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  82% 68/83 [01:41<00:06,  2.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  95% 79/83 [01:40<00:00,  6.43ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11:  99% 82/83 [01:40<00:00,  9.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  87% 72/83 [01:41<00:03,  3.42ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  88% 73/83 [01:42<00:03,  3.30ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  89% 74/83 [01:42<00:02,  3.63ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  92% 76/83 [01:42<00:01,  4.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  93% 77/83 [01:42<00:01,  4.53ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  94% 78/83 [01:43<00:01,  3.46ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  76% 63/83 [01:43<00:20,  1.05s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #11: 100% 83/83 [01:43<00:00,  1.24s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  96% 80/83 [01:44<00:01,  2.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6:  98% 81/83 [01:44<00:00,  2.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  77% 64/83 [01:46<00:25,  1.34s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  75% 62/83 [01:45<00:41,  1.96s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #6: 100% 83/83 [01:46<00:00,  1.29s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  81% 67/83 [01:46<00:13,  1.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  82% 68/83 [01:47<00:12,  1.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  84% 70/83 [01:47<00:07,  1.74ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  77% 64/83 [01:47<00:27,  1.42s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  87% 72/83 [01:47<00:04,  2.22ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  78% 65/83 [01:47<00:22,  1.23s/ex]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  82% 68/83 [01:47<00:10,  1.48ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  84% 70/83 [01:47<00:06,  2.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  86% 71/83 [01:48<00:05,  2.33ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  89% 74/83 [01:48<00:02,  3.76ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  92% 76/83 [01:48<00:01,  3.98ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10:  94% 78/83 [01:48<00:00,  5.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #10: 100% 83/83 [01:49<00:00,  1.32s/ex]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  89% 74/83 [01:51<00:07,  1.22ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  93% 77/83 [01:52<00:03,  1.54ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7:  98% 81/83 [01:52<00:00,  2.58ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Running tokenizer on train dataset #7: 100% 83/83 [01:52<00:00,  1.36s/ex]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:668: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "03/01/2024 11:52:56 - INFO - tevatron.trainer -   Initializing Gradient Cache Trainer\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 35\n",
            " 14% 5/35 [00:26<02:38,  5.28s/it]"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python -m tevatron.driver.train \\\n",
        "  --output_dir dpr_nfcorpus \\\n",
        "  --model_name_or_path $BASE_IR_MODEL \\\n",
        "  --save_steps 10 \\\n",
        "  --dataset_name Tevatron/wikipedia-nq \\\n",
        "  --train_dir /content/nfcorpus.train.jsonl \\\n",
        "  --fp16 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --positive_passage_no_shuffle \\\n",
        "  --train_n_passages 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --q_max_len 32 \\\n",
        "  --p_max_len 156 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --logging_steps 500 \\\n",
        "  --grad_cache \\\n",
        "  --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHfZw1iBIYdV"
      },
      "source": [
        "## 評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CORqroFAIX_j"
      },
      "outputs": [],
      "source": [
        "!bash tevatron/scripts/eval_beir.sh dpr_nfcorpus nfcorpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhpYP6eGe1lG"
      },
      "outputs": [],
      "source": [
        "COLAB_END_TIME = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPTrY6tGg5uj"
      },
      "outputs": [],
      "source": [
        "f\"{(COLAB_END_TIME - COLAB_START_TIME) / 60:.2f}\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}